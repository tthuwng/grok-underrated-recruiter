{
  "handle": "gu_xiangming",
  "bio": "Final-year PhD @NUSingapore, SR @GoogleDeepmind. Prev: @SeaAIL,  @Tsinghua_Uni. Looking for jobs.",
  "followers": 1450,
  "technical_depth": {
    "score": 10,
    "evidence": "ICLR 2025 Spotlight 'When Attention Sink Emerges in Language Models', COLM 2025 'Why Do LLMs Attend to the First Token?', ICML 2024 'Agent Smith' jailbreak, TMLR 2025 diffusion memorization; deep posts on attention sinks, test-time scaling, DeepMind projects."
  },
  "project_evidence": {
    "score": 7,
    "evidence": "GitHub guxm2021 with repos like SVT_SpeechBrain, MM_ALT; contribs to sail-sg/Attention-Sink (143 stars), DiffMemorize (29 stars), Agent-Smith; code for multiple papers but no 1k+ stars or massive scale products."
  },
  "mission_alignment": {
    "score": 10,
    "evidence": "Frequent posts on frontier LLMs: attention sinks (influencing OpenAI GPT-OSS, Qwen NeurIPS best paper), reasoning, scaling, safety; DeepMind SR on LLM understanding package, test-time scaling."
  },
  "exceptional_ability": {
    "score": 10,
    "evidence": "Pioneering attention sink research cited in industry models; collabs with Petar Veli\u010dkovi\u0107, Razvan Pascanu; solved hard problems in LLM pretraining, jailbreaks, memorization; awards like Dean's Graduate Research Excellence."
  },
  "communication": {
    "score": 10,
    "evidence": "Clear X threads explaining attention sink mitigations (softmax-off-by-one, key biases); shared slides; top conf papers (ICLR spotlight, ICML); insightful replies/teaching."
  },
  "final_score": 92.5,
  "summary": "Xiangming Gu is an outstanding final-year PhD with top-tier publications on LLM architectures and safety, current DeepMind Student Researcher, demonstrating exceptional research ability. His work on attention sinks has influenced major models like GPT-OSS and Qwen. Highly suitable for research roles at xAI.",
  "strengths": [
    "Influential frontier LLM research (attention sinks, scaling)",
    "DeepMind experience and top conf publications",
    "Code releases for papers"
  ],
  "concerns": [
    "Limited evidence of large-scale engineering or infra experience",
    "GitHub repos modest in popularity"
  ],
  "recommended_role": "research",
  "github_url": "https://github.com/guxm2021",
  "linkedin_url": "https://sg.linkedin.com/in/xiangming-gu",
  "top_repos": [
    "sail-sg/Attention-Sink",
    "sail-sg/DiffMemorize",
    "guxm2021/MM_ALT"
  ],
  "citations": [
    "https://x.com/gu_xiangming",
    "https://x.com/i/status/1983203287353729310",
    "https://x.com/i/status/1984060183732265469",
    "https://github.com/isXinLiu/Awesome-MLLM-Safety/blob/main/data.json",
    "https://x.com/i/status/1995180353703882858",
    "https://x.com/i/status/1995306084379275732",
    "https://x.com/i/status/1972652964289581265",
    "https://github.com/guxm2021/SVT_SpeechBrain",
    "https://www.linkedin.com/posts/danielle-ong-854b88177_nus-computing-on-instagram-nuscomputings-activity-7056814781344727040-ztiQ",
    "https://www.sotwe.com/gu_xiangming",
    "https://openreview.net/forum?id=78Nn4QJTEN",
    "https://x.com/i/status/1983374416789786981",
    "https://x.com/i/status/1995179981174141176",
    "https://smcnus.comp.nus.edu.sg/archive/pdf/2025/2025_when_attention.pdf",
    "https://x.com/i/status/1952910913482441197",
    "https://smcnus.comp.nus.edu.sg/video_page",
    "https://x.com/i/status/1981003818394354049",
    "https://x.com/i/status/1983200997863543104",
    "https://x.com/i/status/1983207073086124120",
    "https://sites.google.com/view/huanghengguan/services",
    "https://arxiv.org/html/2410.10781v1",
    "https://www.linkedin.com/posts/xiangming-gu_agent-smith-a-single-image-can-jailbreak-activity-7191803085827014657-Ty60",
    "https://x.com/i/status/1975547398773743636",
    "https://x.com/i/status/1953748948306280549",
    "https://x.com/i/status/1917745100576415962",
    "https://arxiv.org/abs/2410.10781",
    "https://x.com/i/status/1972656098881438043",
    "https://proceedings.iclr.cc/paper_files/paper/2025/file/f1b04face60081b689ba740d39ea8f37-Paper-Conference.pdf",
    "https://x.com/i/status/1983203916700627051",
    "https://scholar.google.com/citations?user=BkxEuIoAAAAJ&hl=zh-CN",
    "https://www.linkedin.com/posts/federico-barbero-95b919173_very-excited-to-share-our-paper-on-extracting-activity-7386821693731835905-C2Ve",
    "https://www.linkedin.com/posts/xiangming-gu_welcome-to-cite-our-new-published-paper-about-activity-6827049575304491008-uo71",
    "https://guxm2021.github.io/",
    "https://x.com/i/status/1984036736738193896",
    "https://x.com/i/status/1995169880031805852",
    "https://guxm2021.github.io/pdf/attention_sink_hunyuan_talk.pdf",
    "https://www.linkedin.com/posts/yize-wei-connect_chi2025-activity-7288161536831897601-gl0H",
    "https://github.com/Vincentqyw/cv-arxiv-daily",
    "https://x.com/i/status/1980994748124172539",
    "https://x.com/i/status/1983199724607009066",
    "https://github.com/Trustworthy-AI-Group/Adversarial_Examples_Papers",
    "https://x.com/i/status/1983199969780900083",
    "https://x.com/i/status/1996223908035645908",
    "https://x.com/i/status/1994447631968346140",
    "https://x.com/i/status/1953656755109376040",
    "https://www.linkedin.com/posts/xiangming-gu_icml2024-activity-7221143542939729922-05UK",
    "https://www.linkedin.com/pub/dir/Xiangming/Gu",
    "https://github.com/sail-sg/Attention-Sink",
    "https://liner.com/review/when-attention-sink-emerges-in-language-models-an-empirical-view",
    "https://x.com/i/status/1924832923225883033",
    "https://x.com/i/status/1917547438258450674",
    "https://x.com/i/status/1995181006610165871",
    "https://x.com/i/status/1952811057673642227",
    "https://x.com/i/status/1953023742000415126",
    "https://github.com/liudaizong/Awesome-LVLM-Attack/blob/main/README.md",
    "https://x.com/i/status/1995151989190303972",
    "https://x.com/i/status/1995186714474885279",
    "https://x.com/i/status/1914942584012919238",
    "https://x.com/i/status/1983315567995199864",
    "https://github.com/sail-sg/DiffMemorize",
    "https://x.com/i/status/1993896237083578741",
    "https://github.com/guxm2021/MM_ALT",
    "https://github.com/guxm2021",
    "https://x.com/i/status/1996181423804576107",
    "https://x.com/i/status/1983235254434541855",
    "https://x.com/i/status/1983314357296779529",
    "https://x.com/i/status/1914924013559939170",
    "https://x.com/i/status/1953760480927133896",
    "https://www.alphaxiv.org/ko/overview/2410.10781v1",
    "https://www.linkedin.com/posts/xiangming-gu_when-attention-sink-emerges-in-language-models-activity-7318679091107221504-2Qc_",
    "https://x.com/i/status/1983200430906819046",
    "https://x.com/i/status/1983208094109725100",
    "https://sg.linkedin.com/in/xiangming-gu",
    "https://openreview.net/profile?id=~Xiangming_Gu1",
    "https://x.com/i/status/1975538022814020022",
    "https://x.com/i/status/1968361776321323420",
    "https://x.com/i/status/1995288694333342067",
    "https://x.com/i/status/1968377927999139842",
    "https://github.com/isXinLiu/Awesome-MLLM-Safety/blob/main/README.md",
    "https://x.com/i/status/1983374211692515792",
    "https://x.com/i/status/1994448836559831263",
    "https://x.com/i/status/1952985216810619313",
    "https://x.com/i/status/1953765092643782829",
    "https://x.com/i/status/1993854188171006453",
    "https://x.com/i/status/1983220286603346307",
    "https://www.linkedin.com/pub/dir/Xiangming/%2B"
  ],
  "raw_response": "{\n  \"technical_depth\": {\"score\": 10, \"evidence\": \"ICLR 2025 Spotlight 'When Attention Sink Emerges in Language Models', COLM 2025 'Why Do LLMs Attend to the First Token?', ICML 2024 'Agent Smith' jailbreak, TMLR 2025 diffusion memorization; deep posts on attention sinks, test-time scaling, DeepMind projects.\"},\n  \"project_evidence\": {\"score\": 7, \"evidence\": \"GitHub guxm2021 with repos like SVT_SpeechBrain, MM_ALT; contribs to sail-sg/Attention-Sink (143 stars), DiffMemorize (29 stars), Agent-Smith; code for multiple papers but no 1k+ stars or massive scale products.\"},\n  \"mission_alignment\": {\"score\": 10, \"evidence\": \"Frequent posts on frontier LLMs: attention sinks (influencing OpenAI GPT-OSS, Qwen NeurIPS best paper), reasoning, scaling, safety; DeepMind SR on LLM understanding package, test-time scaling.\"},\n  \"exceptional_ability\": {\"score\": 10, \"evidence\": \"Pioneering attention sink research cited in industry models; collabs with Petar Veli\u010dkovi\u0107, Razvan Pascanu; solved hard problems in LLM pretraining, jailbreaks, memorization; awards like Dean's Graduate Research Excellence.\"},\n  \"communication\": {\"score\": 10, \"evidence\": \"Clear X threads explaining attention sink mitigations (softmax-off-by-one, key biases); shared slides; top conf papers (ICLR spotlight, ICML); insightful replies/teaching.\"},\n  \"summary\": \"Xiangming Gu is an outstanding final-year PhD with top-tier publications on LLM architectures and safety, current DeepMind Student Researcher, demonstrating exceptional research ability. His work on attention sinks has influenced major models like GPT-OSS and Qwen. Highly suitable for research roles at xAI.\",\n  \"strengths\": [\"Influential frontier LLM research (attention sinks, scaling)\", \"DeepMind experience and top conf publications\", \"Code releases for papers\"],\n  \"concerns\": [\"Limited evidence of large-scale engineering or infra experience\", \"GitHub repos modest in popularity\"],\n  \"recommended_role\": \"research\",\n  \"github_url\": \"https://github.com/guxm2021\",\n  \"linkedin_url\": \"https://sg.linkedin.com/in/xiangming-gu\",\n  \"top_repos\": [\"sail-sg/Attention-Sink\", \"sail-sg/DiffMemorize\", \"guxm2021/MM_ALT\"]\n}"
}
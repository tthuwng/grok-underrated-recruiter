{
  "handle": "hmellor_",
  "bio": "ML Engineer @huggingface maintaining @vllm_project, prev @graphcoreai, @uniofoxford",
  "followers": 251,
  "technical_depth": {
    "score": 9,
    "evidence": "Core maintainer of vLLM with implementations of Transformers backend for encoder-only/MoE/sliding window models; co-author on Graphcore paper 'BESS: Balanced Entity Sampling' (arXiv:2211.12281); deep systems knowledge in LLM inference/serving shown in posts/PRs."
  },
  "project_evidence": {
    "score": 10,
    "evidence": "Maintainer/core contributor to vLLM (40k+ stars, 1000+ contributors); shipped features at scale used in production; GitHub https://github.com/hmellor with contributions to high-impact OSS."
  },
  "mission_alignment": {
    "score": 10,
    "evidence": "Posts exclusively about vLLM features, HF integrations, AI serving/inference; talks at Ray Summit/vLLM meetups; obsessed with frontier LLM deployment."
  },
  "exceptional_ability": {
    "score": 9,
    "evidence": "Ownership of major vLLM features (Transformers backend, docs migration Sphinx->MkDocs, installation revamp); solved hard integration/scaling problems in popular OSS; prev Graphcore IPU ML work."
  },
  "communication": {
    "score": 9,
    "evidence": "Clear, engaging X threads announcing features/milestones; extensive docs improvements; responsive technical replies; blog posts at Graphcore."
  },
  "final_score": 94.5,
  "summary": "Harry Mellor is an outstanding ML systems engineer with proven track record maintaining vLLM, a leading open-source LLM serving engine. His deep contributions to high-performance inference make him highly suitable for xAI's infrastructure needs. Background includes Oxford Engineering MEng and Graphcore experience.",
  "strengths": [
    "Expertise in scalable LLM inference systems",
    "Proven OSS leadership and ownership"
  ],
  "concerns": [
    "No public papers as lead author",
    "Relatively low X followers despite impact"
  ],
  "recommended_role": "infrastructure",
  "github_url": "https://github.com/hmellor",
  "linkedin_url": null,
  "top_repos": [
    "vllm-project/vllm"
  ],
  "citations": [
    "https://uk.linkedin.com/in/william-cathery-46270485",
    "https://github.com/hmellor",
    "https://github.com/sponsors/hmellor",
    "https://github.com/hmellor?tab=repositories",
    "https://x.com/i/status/1926196592123453793",
    "https://uk.linkedin.com/in/leo-harris-71a5ab1a2",
    "https://x.com/i/status/1919335076069863884",
    "https://ogb.stanford.edu/paper/neurips2022/wikikg90mv2_wikiwiki.pdf",
    "https://www.linkedin.com/posts/girivenkatesan_mcp-servers-the-technical-debt-that-is-coming-activity-7340597979923300353-82NP",
    "https://news.smol.ai/issues/25-10-17-not-much/",
    "https://github.com/vllm-project/vllm/issues/801",
    "https://x.com/i/status/1973307839289631112",
    "https://www.linkedin.com/posts/embedded-llm_want-hugging-faces-model-variety-and-vllms-activity-7318714694372835328-9S5A",
    "https://x.com/i/status/1979172956078064124",
    "https://ai-engineering-trend.medium.com/vllm-officially-supports-transformers-backend-bert-style-models-get-a-new-lease-on-life-732e4f088867",
    "https://uk.linkedin.com/in/neil-faulkner-542616193",
    "https://www.linkedin.com/posts/avi-chawla_explain-kv-caching-in-llmspopular-interview-activity-7296124409004593153-CizX",
    "https://x.com/i/status/1988620898971083067",
    "https://www.linkedin.com/posts/robert-nishihara-b6465444_ray-summit-2025-activity-7389040707149230080-lPxu",
    "https://x.com/i/status/1896633555650052197",
    "https://github.com/vllm-project/vllm/issues/2733",
    "https://uk.linkedin.com/in/rosie-bithell-6b9391a7",
    "https://www.linkedin.com/posts/arig23498_new-vllm-with-hugging-face-you-can-try-activity-7318681848123928576-oTBp",
    "https://www.linkedin.com/posts/sharechat_realtime-content-deduplication-at-scale-activity-7363178060596051969-4Vpb",
    "https://uk.linkedin.com/in/harry-mellor-6877651a7",
    "https://x.com/i/status/1918609220125024590",
    "https://twitter.com/max_paperclips/",
    "https://video.ibm.com/playlist/656256/video/60025825",
    "https://x.com/i/status/1896586851735286186",
    "https://github.com/vllm-project/vllm/issues/12343",
    "https://x.com/i/status/1896583813645668673",
    "https://graphcore-research.github.io/publications/",
    "https://www.researchgate.net/publication/365661571_BESS_Balanced_Entity_Sampling_and_Sharing_for_Large-Scale_Knowledge_Graph_Completion",
    "https://uk.linkedin.com/in/yixing-wu-b2085088",
    "https://x.com/i/status/1925901325528662364",
    "https://x.com/i/status/1912983028407665108",
    "https://t.co/VI0HJUp0ip,",
    "https://www.youtube.com/watch?v=LnXl5YR7dyA",
    "https://www.instagram.com/harrymellor/?hl=en",
    "https://www.linkedin.com/pub/dir/Harry/Mellor",
    "https://huggingface.co/hmellor/activity/all",
    "https://www.linkedin.com/posts/hellodeolu_kubernetes-cloudnative-devops-activity-7373204824328339456-Gd7a",
    "https://x.com/i/status/1975139761292706132",
    "https://x.com/i/status/1952784776458314058",
    "https://mellor.io/",
    "https://twitter.com/hmellor_/status/1896552868125303077",
    "https://www.politico.eu/newsletter/london-playbook/politico-london-playbook-mind-the-gap-insta-slam-rish-served-cold/",
    "https://github.com/vllm-project/vllm/issues/17989",
    "https://www.graphcore.ai/posts/flan-t5-sweet-results-with-the-smaller-more-efficient-llm",
    "https://www.graphcore.ai/posts/running-flan-t5-xl-inference-in-float16-for-ipu-how-we-did-it",
    "https://github.com/vllm-project/vllm/issues/23451",
    "https://x.com/i/status/1926196995309342913",
    "https://twitter.com/RedHat_AI/",
    "https://x.com/i/status/1975522499644248099",
    "https://x.com/i/status/1896552868125303077",
    "https://x.com/i/status/1973307836584304913",
    "https://huggingface.co/hmellor",
    "https://x.com/i/status/1879618931830497467",
    "https://pe.linkedin.com/in/ccortezcorrales",
    "https://x.com/i/status/1910312939120107626",
    "https://www.linkedin.com/posts/amit-arora-539120a_mcp-mcpregistry-ai-activity-7351287004002775040-0Dnw",
    "https://arxiv.org/abs/2211.12281",
    "https://x.com/i/status/1896685997729173800",
    "https://github.com/vllm-project/vllm/issues/22932",
    "https://x.com/hmellor_/status/1975522499644248099",
    "https://x.com/i/status/1925870103154798930",
    "https://x.com/i/status/1926195317268943304",
    "https://www.graphcore.ai/posts/graphcore-claims-double-win-in-open-graph-benchmark-challenge",
    "https://x.com/i/status/1906665949530366169",
    "https://x.com/hmellor_/status/1979172956078064124",
    "https://uk.linkedin.com/in/prasanna-ponmudi-2ab57417",
    "https://x.com/i/status/1879618934808400335",
    "https://github.com/orgs/vllm-project/people",
    "https://x.com/i/status/1973984174714597604",
    "https://twstalker.com/ben_burtenshaw",
    "https://huggingface.co/hmellor/collections",
    "https://uk.linkedin.com/in/court-harding-6a8aaa112",
    "https://github.com/vllm-project/vllm/issues/16012",
    "https://x.com/i/status/1926200010154152218",
    "https://uk.linkedin.com/in/nick-parsons-84220a43",
    "https://huggingface.co/Graphcore",
    "https://www.researchgate.net/profile/Harry-Mellor",
    "https://x.com/i/status/1912973236523930000",
    "https://scholar.google.com/citations?user=5L3QDRYAAAAJ&hl=en",
    "https://www.st-hughs.ox.ac.uk/wp-content/uploads/2024/01/St-Hughs-Chronicle-2017m-aw-compressed.pdf",
    "https://github.com/vllm-project/vllm/graphs/contributors",
    "https://x.com/i/status/1988205999170392288",
    "https://x.com/i/status/1879618929724973319",
    "https://x.com/i/status/1926192864502898901",
    "https://hud.pytorch.org/vllm-project/vllm/commit/840cc58fe893d6aea96d2c6b7af127cde1d87522",
    "https://x.com/hmellor_"
  ],
  "raw_response": "{\n  \"technical_depth\": {\"score\": 9, \"evidence\": \"Core maintainer of vLLM with implementations of Transformers backend for encoder-only/MoE/sliding window models; co-author on Graphcore paper 'BESS: Balanced Entity Sampling' (arXiv:2211.12281); deep systems knowledge in LLM inference/serving shown in posts/PRs.\"},\n  \"project_evidence\": {\"score\": 10, \"evidence\": \"Maintainer/core contributor to vLLM (40k+ stars, 1000+ contributors); shipped features at scale used in production; GitHub https://github.com/hmellor with contributions to high-impact OSS.\"},\n  \"mission_alignment\": {\"score\": 10, \"evidence\": \"Posts exclusively about vLLM features, HF integrations, AI serving/inference; talks at Ray Summit/vLLM meetups; obsessed with frontier LLM deployment.\"},\n  \"exceptional_ability\": {\"score\": 9, \"evidence\": \"Ownership of major vLLM features (Transformers backend, docs migration Sphinx->MkDocs, installation revamp); solved hard integration/scaling problems in popular OSS; prev Graphcore IPU ML work.\"},\n  \"communication\": {\"score\": 9, \"evidence\": \"Clear, engaging X threads announcing features/milestones; extensive docs improvements; responsive technical replies; blog posts at Graphcore.\"},\n  \"summary\": \"Harry Mellor is an outstanding ML systems engineer with proven track record maintaining vLLM, a leading open-source LLM serving engine. His deep contributions to high-performance inference make him highly suitable for xAI's infrastructure needs. Background includes Oxford Engineering MEng and Graphcore experience.\",\n  \"strengths\": [\"Expertise in scalable LLM inference systems\", \"Proven OSS leadership and ownership\"],\n  \"concerns\": [\"No public papers as lead author\", \"Relatively low X followers despite impact\"],\n  \"recommended_role\": \"infrastructure\",\n  \"github_url\": \"https://github.com/hmellor\",\n  \"linkedin_url\": null,\n  \"top_repos\": [\"vllm-project/vllm\"]\n}"
}
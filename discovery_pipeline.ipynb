{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grok Recruiter - Taste-Graph Talent Discovery\n",
    "\n",
    "This notebook walks through each phase of the discovery pipeline:\n",
    "\n",
    "1. **Setup** - Load config and initialize clients\n",
    "2. **Phase 1** - Resolve seed accounts (xAI employees)\n",
    "3. **Phase 2** - Expand graph (following > likes > retweets > replies)\n",
    "4. **Phase 3** - Hydrate user profiles & filter candidates\n",
    "5. **Phase 4a** - Fast LLM screening (bio + pinned tweet)\n",
    "6. **Phase 4b** - PageRank ranking (on fast-screened candidates)\n",
    "7. **Phase 5** - Deep Evaluation with xAI Search Tools\n",
    "8. **Phase 6** - Export & UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Load Config & Initialize Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import our modules\n",
    "from src.x_client import XClient\n",
    "from src.grok_client import GrokClient, load_criteria\n",
    "from src.graph_builder import GraphBuilder, Node, Edge\n",
    "from src.evaluator import CandidateEvaluator, PreFilterResult, print_evaluation_summary\n",
    "from src.ranking import compute_rankings, export_ranked_nodes\n",
    "\n",
    "print(\"Modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 90 seed accounts\n",
      "Settings: {'max_depth': 1, 'max_followers_candidate': 50000, 'min_followers_candidate': 50, 'max_following_per_root': 500, 'max_liked_tweets': 100, 'max_root_tweets': 50, 'max_likers_per_tweet': 100, 'max_retweeters_per_tweet': 100, 'max_replies_per_conversation': 50}\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "CONFIG_PATH = \"seeds.yaml\"\n",
    "CRITERIA_PATH = \"criteria.yaml\"\n",
    "CACHE_DIR = \"data/raw\"\n",
    "EVAL_CACHE_DIR = \"data/evaluations\"\n",
    "OUTPUT_DIR = \"data/processed\"\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "roots = config.get(\"roots\", [])\n",
    "settings = config.get(\"settings\", {})\n",
    "\n",
    "print(f\"Loaded {len(roots)} seed accounts\")\n",
    "print(f\"Settings: {settings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X API client initialized\n",
      "Grok client initialized\n",
      "  Fast model: grok-4-1-fast-non-reasoning\n",
      "  Full model: grok-4-1-fast-reasoning\n"
     ]
    }
   ],
   "source": [
    "# Initialize X API client\n",
    "x_client = XClient(cache_dir=CACHE_DIR)\n",
    "print(\"X API client initialized\")\n",
    "\n",
    "# Initialize Grok client\n",
    "grok_client = GrokClient(cache_dir=EVAL_CACHE_DIR)\n",
    "print(\"Grok client initialized\")\n",
    "print(f\"  Fast model: {grok_client.fast_model}\")\n",
    "print(f\"  Full model: {grok_client.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 - Resolve Seed Accounts\n",
    "\n",
    "Convert seed handles to user objects with IDs and metadata.\n",
    "\n",
    "**No LLM calls** - just X API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph builder initialized with 90 roots\n"
     ]
    }
   ],
   "source": [
    "# Initialize graph builder\n",
    "builder = GraphBuilder(\n",
    "    x_client=x_client,\n",
    "    roots=roots,\n",
    "    settings=settings,\n",
    ")\n",
    "\n",
    "print(f\"Graph builder initialized with {len(roots)} roots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 1: Resolving root accounts\n",
      "============================================================\n",
      "  Fetching user: @elonmusk\n",
      "  [cache] Loading from user_elonmusk.json\n",
      "  [root] @elonmusk (ID: 44196397, followers: 229,420,382)\n",
      "  Fetching user: @A5_Wagyu\n",
      "  [cache] Loading from user_A5_Wagyu.json\n",
      "  [root] @A5_Wagyu (ID: 265152280, followers: 409)\n",
      "  Fetching user: @APrerepa\n",
      "  [cache] Loading from user_APrerepa.json\n",
      "  [root] @APrerepa (ID: 1146180676091170817, followers: 2,254)\n",
      "  Fetching user: @AdamSliwakowski\n",
      "  [cache] Loading from user_AdamSliwakowski.json\n",
      "  [root] @AdamSliwakowski (ID: 1513813650, followers: 371)\n",
      "  Fetching user: @a2xai\n",
      "  [cache] Loading from user_a2xai.json\n",
      "  [root] @a2xai (ID: 1173974870, followers: 2,081)\n",
      "  Fetching user: @adityagupta\n",
      "  [cache] Loading from user_adityagupta.json\n",
      "  [root] @adityagupta (ID: 2269061347, followers: 7,935)\n",
      "  Fetching user: @aknowntheory\n",
      "  [cache] Loading from user_aknowntheory.json\n",
      "  [root] @aknowntheory (ID: 630619183, followers: 516)\n",
      "  Fetching user: @Alexjpeng\n",
      "  [cache] Loading from user_Alexjpeng.json\n",
      "  [root] @Alexjpeng (ID: 67784137, followers: 252)\n",
      "  Fetching user: @alsiens\n",
      "  [cache] Loading from user_alsiens.json\n",
      "  [root] @alsiens (ID: 624650524, followers: 2,074)\n",
      "  Fetching user: @AndyKim99999\n",
      "  [cache] Loading from user_AndyKim99999.json\n",
      "  [root] @AndyKim99999 (ID: 1891958666267676672, followers: 45)\n",
      "  Fetching user: @anzhit\n",
      "  [cache] Loading from user_anzhit.json\n",
      "  [root] @anzhit (ID: 54136863, followers: 259)\n",
      "  Fetching user: @ArashBidgoli\n",
      "  [cache] Loading from user_ArashBidgoli.json\n",
      "  [root] @ArashBidgoli (ID: 923582708344938496, followers: 1,098)\n",
      "  Fetching user: @arnab_iitk\n",
      "  [cache] Loading from user_arnab_iitk.json\n",
      "  [root] @arnab_iitk (ID: 1270744574, followers: 252)\n",
      "  Fetching user: @ArnoCandel\n",
      "  [cache] Loading from user_ArnoCandel.json\n",
      "  [root] @ArnoCandel (ID: 515145898, followers: 5,161)\n",
      "  Fetching user: @attilablenesi\n",
      "  [cache] Loading from user_attilablenesi.json\n",
      "  [root] @attilablenesi (ID: 282176468, followers: 10,972)\n",
      "  Fetching user: @b0ch4r0v\n",
      "  [cache] Loading from user_b0ch4r0v.json\n",
      "  [root] @b0ch4r0v (ID: 584832148, followers: 666)\n",
      "  Fetching user: @BillyuChenLin\n",
      "  [cache] Loading from user_BillyuChenLin.json\n",
      "  [root] @BillyuChenLin (ID: 726053744731807744, followers: 25,888)\n",
      "  Fetching user: @billyuchenlin\n",
      "  [cache] Loading from user_billyuchenlin.json\n",
      "  [root] @billyuchenlin (ID: 726053744731807744, followers: 25,888)\n",
      "  Fetching user: @cph816\n",
      "  [cache] Loading from user_cph816.json\n",
      "  [root] @cph816 (ID: 934741229413261313, followers: 127)\n",
      "  Fetching user: @CrisGiardina\n",
      "  [cache] Loading from user_CrisGiardina.json\n",
      "  [root] @CrisGiardina (ID: 11373532, followers: 6,361)\n",
      "  Fetching user: @CtrlAltHero\n",
      "  [cache] Loading from user_CtrlAltHero.json\n",
      "  [root] @CtrlAltHero (ID: 1897824925588897797, followers: 312)\n",
      "  Fetching user: @DMananniko56701\n",
      "  [cache] Loading from user_DMananniko56701.json\n",
      "  [root] @DMananniko56701 (ID: 1879724460644958208, followers: 233)\n",
      "  Fetching user: @dma_running\n",
      "  [cache] Loading from user_dma_running.json\n",
      "  [root] @dma_running (ID: 1909342964826226689, followers: 1,078)\n",
      "  Fetching user: @DrakeJWong\n",
      "  [cache] Loading from user_DrakeJWong.json\n",
      "  [root] @DrakeJWong (ID: 703664678, followers: 367)\n",
      "  Fetching user: @dustinvtran\n",
      "  [cache] Loading from user_dustinvtran.json\n",
      "  [root] @dustinvtran (ID: 1540897980, followers: 57,236)\n",
      "  Fetching user: @edwinboyan\n",
      "  [cache] Loading from user_edwinboyan.json\n",
      "  [root] @edwinboyan (ID: 208364506, followers: 855)\n",
      "  Fetching user: @El1flem\n",
      "  [cache] Loading from user_El1flem.json\n",
      "  [root] @El1flem (ID: 1103694001, followers: 388)\n",
      "  Fetching user: @endernewton\n",
      "  [cache] Loading from user_endernewton.json\n",
      "  [root] @endernewton (ID: 334448097, followers: 2,908)\n",
      "  Fetching user: @ericli_\n",
      "  [cache] Loading from user_ericli_.json\n",
      "  [root] @ericli_ (ID: 1880069842696294400, followers: 1,083)\n",
      "  Fetching user: @fraser_cook\n",
      "  [cache] Loading from user_fraser_cook.json\n",
      "  [root] @fraser_cook (ID: 213141794, followers: 850)\n",
      "  Fetching user: @GrahamSeamans\n",
      "  [cache] Loading from user_GrahamSeamans.json\n",
      "  [root] @GrahamSeamans (ID: 327111999, followers: 369)\n",
      "  Fetching user: @GrantEMatheson\n",
      "  [cache] Loading from user_GrantEMatheson.json\n",
      "  [root] @GrantEMatheson (ID: 592470094, followers: 292)\n",
      "  Fetching user: @TheGregYang\n",
      "  [cache] Loading from user_TheGregYang.json\n",
      "  [root] @TheGregYang (ID: 1092693586263457792, followers: 93,467)\n",
      "  Fetching user: @Guodzh\n",
      "  [cache] Loading from user_Guodzh.json\n",
      "  [root] @Guodzh (ID: 734677275216470016, followers: 33,393)\n",
      "  Fetching user: @haowengeget\n",
      "  [cache] Loading from user_haowengeget.json\n",
      "  [root] @haowengeget (ID: 1170710434216849408, followers: 447)\n",
      "  Fetching user: @hexianghu\n",
      "  [cache] Loading from user_hexianghu.json\n",
      "  [root] @hexianghu (ID: 2786462342, followers: 3,348)\n",
      "  Fetching user: @houndstongue\n",
      "  [cache] Loading from user_houndstongue.json\n",
      "  [root] @houndstongue (ID: 946880235680677889, followers: 65)\n",
      "  Fetching user: @imhaotian\n",
      "  [cache] Loading from user_imhaotian.json\n",
      "  [root] @imhaotian (ID: 2267475408, followers: 11,362)\n",
      "  Fetching user: @ironsoul0\n",
      "  [cache] Loading from user_ironsoul0.json\n",
      "  [root] @ironsoul0 (ID: 1158146842875633664, followers: 282)\n",
      "  Fetching user: @jaesungtae\n",
      "  [cache] Loading from user_jaesungtae.json\n",
      "  [root] @jaesungtae (ID: 1069621025493737473, followers: 199)\n",
      "  Fetching user: @jitrc\n",
      "  [cache] Loading from user_jitrc.json\n",
      "  [root] @jitrc (ID: 32488751, followers: 459)\n",
      "  Fetching user: @JohnBoccio\n",
      "  [cache] Loading from user_JohnBoccio.json\n",
      "  [root] @JohnBoccio (ID: 564296919, followers: 2,101)\n",
      "  Fetching user: @JohnOn1337\n",
      "  [cache] Loading from user_JohnOn1337.json\n",
      "  [root] @JohnOn1337 (ID: 558012861, followers: 141)\n",
      "  Fetching user: @jonnylangefeld\n",
      "  [cache] Loading from user_jonnylangefeld.json\n",
      "  [root] @jonnylangefeld (ID: 521427555, followers: 1,199)\n",
      "  Fetching user: @kevngg\n",
      "  [cache] Loading from user_kevngg.json\n",
      "  [root] @kevngg (ID: 770957318485749761, followers: 580)\n",
      "  Fetching user: @KyleZaragoza\n",
      "  [cache] Loading from user_KyleZaragoza.json\n",
      "  [root] @KyleZaragoza (ID: 74017055, followers: 1,949)\n",
      "  Fetching user: @larrabearr\n",
      "  [cache] Loading from user_larrabearr.json\n",
      "  [root] @larrabearr (ID: 46636556, followers: 2,599)\n",
      "  Fetching user: @LiangchenLuo\n",
      "  [cache] Loading from user_LiangchenLuo.json\n",
      "  [root] @LiangchenLuo (ID: 931209924385710080, followers: 6,233)\n",
      "  Fetching user: @lm_zheng\n",
      "  [cache] Loading from user_lm_zheng.json\n",
      "  [root] @lm_zheng (ID: 952696452173594625, followers: 14,526)\n",
      "  Fetching user: @mindbergh\n",
      "  [cache] Loading from user_mindbergh.json\n",
      "  [root] @mindbergh (ID: 1037380212, followers: 987)\n",
      "  Fetching user: @MohitMittal26\n",
      "  [cache] Loading from user_MohitMittal26.json\n",
      "  [root] @MohitMittal26 (ID: 1992713924605825025, followers: 16)\n",
      "  Fetching user: @nmswede\n",
      "  [cache] Loading from user_nmswede.json\n",
      "  [root] @nmswede (ID: 74834469, followers: 11,806)\n",
      "  Fetching user: @Nitemaeric\n",
      "  [cache] Loading from user_Nitemaeric.json\n",
      "  [root] @Nitemaeric (ID: 275625421, followers: 346)\n",
      "  Fetching user: @onhachoe1\n",
      "  [cache] Loading from user_onhachoe1.json\n",
      "  [root] @onhachoe1 (ID: 55629516, followers: 710)\n",
      "  Fetching user: @pablo_pensees\n",
      "  [cache] Loading from user_pablo_pensees.json\n",
      "  [root] @pablo_pensees (ID: 2300690840, followers: 832)\n",
      "  Fetching user: @pengshangfu\n",
      "  [cache] Loading from user_pengshangfu.json\n",
      "  [root] @pengshangfu (ID: 347198527, followers: 27)\n",
      "  Fetching user: @phroo\n",
      "  [cache] Loading from user_phroo.json\n",
      "  [root] @phroo (ID: 313672707, followers: 2,925)\n",
      "  Fetching user: @prolificgoober\n",
      "  [cache] Loading from user_prolificgoober.json\n",
      "  [root] @prolificgoober (ID: 2563553689, followers: 108)\n",
      "  Fetching user: @rabahtahraoui\n",
      "  [cache] Loading from user_rabahtahraoui.json\n",
      "  [root] @rabahtahraoui (ID: 1120343086029209600, followers: 159)\n",
      "  Fetching user: @Randxie29\n",
      "  [cache] Loading from user_Randxie29.json\n",
      "  [root] @Randxie29 (ID: 767005845813026816, followers: 335)\n",
      "  Fetching user: @rayhotate\n",
      "  [cache] Loading from user_rayhotate.json\n",
      "  [root] @rayhotate (ID: 917036353832984578, followers: 13,892)\n",
      "  Fetching user: @riley_trett43\n",
      "  [cache] Loading from user_riley_trett43.json\n",
      "  [root] @riley_trett43 (ID: 1135283251, followers: 446)\n",
      "  Fetching user: @rochan_hm\n",
      "  [cache] Loading from user_rochan_hm.json\n",
      "  [root] @rochan_hm (ID: 1280949979, followers: 93)\n",
      "  Fetching user: @rpoo\n",
      "  [cache] Loading from user_rpoo.json\n",
      "  [root] @rpoo (ID: 314341900, followers: 38,178)\n",
      "  Fetching user: @RonghangHu\n",
      "  [cache] Loading from user_RonghangHu.json\n",
      "  [root] @RonghangHu (ID: 2757325333, followers: 422)\n",
      "  Fetching user: @Saaaang94\n",
      "  [cache] Loading from user_Saaaang94.json\n",
      "  [root] @Saaaang94 (ID: 781024050848161796, followers: 2,821)\n",
      "  Fetching user: @sahiljain314\n",
      "  [cache] Loading from user_sahiljain314.json\n",
      "  [root] @sahiljain314 (ID: 1099914793866035200, followers: 1,665)\n",
      "  Fetching user: @satu0king\n",
      "  [cache] Loading from user_satu0king.json\n",
      "  [root] @satu0king (ID: 1356657338, followers: 234)\n",
      "  Fetching user: @shengs1123\n",
      "  [cache] Loading from user_shengs1123.json\n",
      "  [root] @shengs1123 (ID: 1103577756909023232, followers: 2,414)\n",
      "  Fetching user: @shefys\n",
      "  [cache] Loading from user_shefys.json\n",
      "  [root] @shefys (ID: 11451052, followers: 544)\n",
      "  Fetching user: @sishaar\n",
      "  [cache] Loading from user_sishaar.json\n",
      "  [root] @sishaar (ID: 578176008, followers: 1,096)\n",
      "  Fetching user: @skory_by\n",
      "  [cache] Loading from user_skory_by.json\n",
      "  [root] @skory_by (ID: 1394449160, followers: 30)\n",
      "  Fetching user: @spencersharkey\n",
      "  [cache] Loading from user_spencersharkey.json\n",
      "  [root] @spencersharkey (ID: 2345951964, followers: 1,421)\n",
      "  Fetching user: @stepango\n",
      "  [cache] Loading from user_stepango.json\n",
      "  [root] @stepango (ID: 70057560, followers: 562)\n",
      "  Fetching user: @StewartSlocum1\n",
      "  [cache] Loading from user_StewartSlocum1.json\n",
      "  [root] @StewartSlocum1 (ID: 1172647422587064322, followers: 375)\n",
      "  Fetching user: @textangel\n",
      "  [cache] Loading from user_textangel.json\n",
      "  [root] @textangel (ID: 105961577, followers: 1,151)\n",
      "  Fetching user: @thebigreesh\n",
      "  [cache] Loading from user_thebigreesh.json\n",
      "  [root] @thebigreesh (ID: 2439038563, followers: 235)\n",
      "  Fetching user: @ThePenguinCo\n",
      "  [cache] Loading from user_ThePenguinCo.json\n",
      "  [root] @ThePenguinCo (ID: 2427750102, followers: 605)\n",
      "  Fetching user: @TobyPhln\n",
      "  [cache] Loading from user_TobyPhln.json\n",
      "  [root] @TobyPhln (ID: 1023014000, followers: 48,303)\n",
      "  Fetching user: @tylerkniess\n",
      "  [cache] Loading from user_tylerkniess.json\n",
      "  [root] @tylerkniess (ID: 80460759, followers: 310)\n",
      "  Fetching user: @tylervstorm\n",
      "  [cache] Loading from user_tylervstorm.json\n",
      "  [root] @tylervstorm (ID: 899720210084765696, followers: 7,431)\n",
      "  Fetching user: @WillFuxAI\n",
      "  [cache] Loading from user_WillFuxAI.json\n",
      "  [root] @WillFuxAI (ID: 1997353872688635904, followers: 0)\n",
      "  Fetching user: @wyudun\n",
      "  [cache] Loading from user_wyudun.json\n",
      "  [root] @wyudun (ID: 2449279502, followers: 859)\n",
      "  Fetching user: @x_hanyu\n",
      "  [cache] Loading from user_x_hanyu.json\n",
      "  [root] @x_hanyu (ID: 2764212415, followers: 314)\n",
      "  Fetching user: @xiaobinwu\n",
      "  [cache] Loading from user_xiaobinwu.json\n",
      "  [root] @xiaobinwu (ID: 99208056, followers: 1,149)\n",
      "  Fetching user: @Yang1fan2\n",
      "  [cache] Loading from user_Yang1fan2.json\n",
      "  [root] @Yang1fan2 (ID: 332658158, followers: 141)\n",
      "  Fetching user: @YangNaruto\n",
      "  [cache] Loading from user_YangNaruto.json\n",
      "  [root] @YangNaruto (ID: 912642665870774273, followers: 66)\n",
      "  Fetching user: @yiwenyuan98\n",
      "  [cache] Loading from user_yiwenyuan98.json\n",
      "  [root] @yiwenyuan98 (ID: 1093044471447646209, followers: 5,717)\n",
      "  Fetching user: @zhangir_azerbay\n",
      "  [cache] Loading from user_zhangir_azerbay.json\n",
      "  [root] @zhangir_azerbay (ID: 1148267279718801408, followers: 3,497)\n",
      "  Fetching user: @zheweishi\n",
      "  [cache] Loading from user_zheweishi.json\n",
      "  [root] @zheweishi (ID: 2734470248, followers: 116)\n",
      "\n",
      "Resolved 90/90 root accounts\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Resolve root accounts\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: Resolving root accounts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "root_users = {}\n",
    "\n",
    "for handle in roots:\n",
    "    user = x_client.get_user_by_username(handle)\n",
    "    if not user:\n",
    "        print(f\"  [warn] Could not resolve @{handle}\")\n",
    "        continue\n",
    "    \n",
    "    root_users[handle] = user\n",
    "    builder._add_or_update_node(user, is_root=True)\n",
    "    followers = user.get('public_metrics', {}).get('followers_count', 0)\n",
    "    print(f\"  [root] @{handle} (ID: {user['id']}, followers: {followers:,})\")\n",
    "\n",
    "print(f\"\\nResolved {len(root_users)}/{len(roots)} root accounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 - Expand Graph\n",
    "\n",
    "Expand from seeds using priority order:\n",
    "1. **Following** (weight 5.0) - Who seeds chose to follow\n",
    "2. **Retweets** (weight 3.0) - Who retweeted seed tweets\n",
    "3. **Replies** (weight 2.5) - Who replied to seed tweets\n",
    "\n",
    "**Note:** Likes endpoint not available on Free/Basic X API tier (skipped)\n",
    "\n",
    "**No LLM calls** - just X API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 2a: Expanding via FOLLOWING (strongest signal)\n",
      "============================================================\n",
      "\n",
      "[root] @elonmusk - fetching following...\n",
      "  Fetching following for user 44196397\n",
      "  [cache] Loading from following_44196397_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @A5_Wagyu - fetching following...\n",
      "  Fetching following for user 265152280\n",
      "  [cache] Loading from following_265152280_page0.json\n",
      "    Found 480 accounts followed\n",
      "  [following] Processing 480 accounts followed by root\n",
      "\n",
      "[root] @APrerepa - fetching following...\n",
      "  Fetching following for user 1146180676091170817\n",
      "  [cache] Loading from following_1146180676091170817_page0.json\n",
      "    Found 378 accounts followed\n",
      "  [following] Processing 378 accounts followed by root\n",
      "\n",
      "[root] @AdamSliwakowski - fetching following...\n",
      "  Fetching following for user 1513813650\n",
      "  [cache] Loading from following_1513813650_page0.json\n",
      "    Found 491 accounts followed\n",
      "  [following] Processing 491 accounts followed by root\n",
      "\n",
      "[root] @a2xai - fetching following...\n",
      "  Fetching following for user 1173974870\n",
      "  [cache] Loading from following_1173974870_page0.json\n",
      "    Found 116 accounts followed\n",
      "  [following] Processing 116 accounts followed by root\n",
      "\n",
      "[root] @adityagupta - fetching following...\n",
      "  Fetching following for user 2269061347\n",
      "  [cache] Loading from following_2269061347_page0.json\n",
      "    Found 257 accounts followed\n",
      "  [following] Processing 257 accounts followed by root\n",
      "\n",
      "[root] @aknowntheory - fetching following...\n",
      "  Fetching following for user 630619183\n",
      "  [cache] Loading from following_630619183_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @Alexjpeng - fetching following...\n",
      "  Fetching following for user 67784137\n",
      "  [cache] Loading from following_67784137_page0.json\n",
      "    Found 285 accounts followed\n",
      "  [following] Processing 285 accounts followed by root\n",
      "\n",
      "[root] @alsiens - fetching following...\n",
      "  Fetching following for user 624650524\n",
      "  [cache] Loading from following_624650524_page0.json\n",
      "    Found 328 accounts followed\n",
      "  [following] Processing 328 accounts followed by root\n",
      "\n",
      "[root] @AndyKim99999 - fetching following...\n",
      "  Fetching following for user 1891958666267676672\n",
      "  [cache] Loading from following_1891958666267676672_page0.json\n",
      "    Found 3 accounts followed\n",
      "  [following] Processing 3 accounts followed by root\n",
      "\n",
      "[root] @anzhit - fetching following...\n",
      "  Fetching following for user 54136863\n",
      "  [cache] Loading from following_54136863_page0.json\n",
      "    Found 296 accounts followed\n",
      "  [following] Processing 296 accounts followed by root\n",
      "\n",
      "[root] @ArashBidgoli - fetching following...\n",
      "  Fetching following for user 923582708344938496\n",
      "  [cache] Loading from following_923582708344938496_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @arnab_iitk - fetching following...\n",
      "  Fetching following for user 1270744574\n",
      "  [cache] Loading from following_1270744574_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @ArnoCandel - fetching following...\n",
      "  Fetching following for user 515145898\n",
      "  [cache] Loading from following_515145898_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @attilablenesi - fetching following...\n",
      "  Fetching following for user 282176468\n",
      "  [cache] Loading from following_282176468_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @b0ch4r0v - fetching following...\n",
      "  Fetching following for user 584832148\n",
      "  [cache] Loading from following_584832148_page0.json\n",
      "    Found 152 accounts followed\n",
      "  [following] Processing 152 accounts followed by root\n",
      "\n",
      "[root] @BillyuChenLin - fetching following...\n",
      "  Fetching following for user 726053744731807744\n",
      "  [cache] Loading from following_726053744731807744_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @billyuchenlin - fetching following...\n",
      "  Fetching following for user 726053744731807744\n",
      "  [cache] Loading from following_726053744731807744_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @cph816 - fetching following...\n",
      "  Fetching following for user 934741229413261313\n",
      "  [cache] Loading from following_934741229413261313_page0.json\n",
      "    Found 14 accounts followed\n",
      "  [following] Processing 14 accounts followed by root\n",
      "\n",
      "[root] @CrisGiardina - fetching following...\n",
      "  Fetching following for user 11373532\n",
      "  [cache] Loading from following_11373532_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @CtrlAltHero - fetching following...\n",
      "  Fetching following for user 1897824925588897797\n",
      "  [cache] Loading from following_1897824925588897797_page0.json\n",
      "    Found 307 accounts followed\n",
      "  [following] Processing 307 accounts followed by root\n",
      "\n",
      "[root] @DMananniko56701 - fetching following...\n",
      "  Fetching following for user 1879724460644958208\n",
      "  [cache] Loading from following_1879724460644958208_page0.json\n",
      "    Found 3 accounts followed\n",
      "  [following] Processing 3 accounts followed by root\n",
      "\n",
      "[root] @dma_running - fetching following...\n",
      "  Fetching following for user 1909342964826226689\n",
      "  [cache] Loading from following_1909342964826226689_page0.json\n",
      "    Found 18 accounts followed\n",
      "  [following] Processing 18 accounts followed by root\n",
      "\n",
      "[root] @DrakeJWong - fetching following...\n",
      "  Fetching following for user 703664678\n",
      "  [cache] Loading from following_703664678_page0.json\n",
      "    Found 30 accounts followed\n",
      "  [following] Processing 30 accounts followed by root\n",
      "\n",
      "[root] @dustinvtran - fetching following...\n",
      "  Fetching following for user 1540897980\n",
      "  [cache] Loading from following_1540897980_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @edwinboyan - fetching following...\n",
      "  Fetching following for user 208364506\n",
      "  [cache] Loading from following_208364506_page0.json\n",
      "    Found 150 accounts followed\n",
      "  [following] Processing 150 accounts followed by root\n",
      "\n",
      "[root] @El1flem - fetching following...\n",
      "  Fetching following for user 1103694001\n",
      "  [cache] Loading from following_1103694001_page0.json\n",
      "    Found 122 accounts followed\n",
      "  [following] Processing 122 accounts followed by root\n",
      "\n",
      "[root] @endernewton - fetching following...\n",
      "  Fetching following for user 334448097\n",
      "  [cache] Loading from following_334448097_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @ericli_ - fetching following...\n",
      "  Fetching following for user 1880069842696294400\n",
      "  [cache] Loading from following_1880069842696294400_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @fraser_cook - fetching following...\n",
      "  Fetching following for user 213141794\n",
      "  [cache] Loading from following_213141794_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @GrahamSeamans - fetching following...\n",
      "  Fetching following for user 327111999\n",
      "  [cache] Loading from following_327111999_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @GrantEMatheson - fetching following...\n",
      "  Fetching following for user 592470094\n",
      "  [cache] Loading from following_592470094_page0.json\n",
      "    Found 265 accounts followed\n",
      "  [following] Processing 265 accounts followed by root\n",
      "\n",
      "[root] @TheGregYang - fetching following...\n",
      "  Fetching following for user 1092693586263457792\n",
      "  [cache] Loading from following_1092693586263457792_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @Guodzh - fetching following...\n",
      "  Fetching following for user 734677275216470016\n",
      "  [cache] Loading from following_734677275216470016_page0.json\n",
      "    Found 473 accounts followed\n",
      "  [following] Processing 473 accounts followed by root\n",
      "\n",
      "[root] @haowengeget - fetching following...\n",
      "  Fetching following for user 1170710434216849408\n",
      "  [cache] Loading from following_1170710434216849408_page0.json\n",
      "    Found 187 accounts followed\n",
      "  [following] Processing 187 accounts followed by root\n",
      "\n",
      "[root] @hexianghu - fetching following...\n",
      "  Fetching following for user 2786462342\n",
      "  [cache] Loading from following_2786462342_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @houndstongue - fetching following...\n",
      "  Fetching following for user 946880235680677889\n",
      "  [cache] Loading from following_946880235680677889_page0.json\n",
      "    Found 115 accounts followed\n",
      "  [following] Processing 115 accounts followed by root\n",
      "\n",
      "[root] @imhaotian - fetching following...\n",
      "  Fetching following for user 2267475408\n",
      "  [cache] Loading from following_2267475408_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @ironsoul0 - fetching following...\n",
      "  Fetching following for user 1158146842875633664\n",
      "  [cache] Loading from following_1158146842875633664_page0.json\n",
      "    Found 386 accounts followed\n",
      "  [following] Processing 386 accounts followed by root\n",
      "\n",
      "[root] @jaesungtae - fetching following...\n",
      "  Fetching following for user 1069621025493737473\n",
      "  [cache] Loading from following_1069621025493737473_page0.json\n",
      "    Found 305 accounts followed\n",
      "  [following] Processing 305 accounts followed by root\n",
      "\n",
      "[root] @jitrc - fetching following...\n",
      "  Fetching following for user 32488751\n",
      "  [cache] Loading from following_32488751_page0.json\n",
      "    Found 98 accounts followed\n",
      "  [following] Processing 98 accounts followed by root\n",
      "\n",
      "[root] @JohnBoccio - fetching following...\n",
      "  Fetching following for user 564296919\n",
      "  [cache] Loading from following_564296919_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @JohnOn1337 - fetching following...\n",
      "  Fetching following for user 558012861\n",
      "  [cache] Loading from following_558012861_page0.json\n",
      "    Found 48 accounts followed\n",
      "  [following] Processing 48 accounts followed by root\n",
      "\n",
      "[root] @jonnylangefeld - fetching following...\n",
      "  Fetching following for user 521427555\n",
      "  [cache] Loading from following_521427555_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @kevngg - fetching following...\n",
      "  Fetching following for user 770957318485749761\n",
      "  [cache] Loading from following_770957318485749761_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @KyleZaragoza - fetching following...\n",
      "  Fetching following for user 74017055\n",
      "  [cache] Loading from following_74017055_page0.json\n",
      "    Found 265 accounts followed\n",
      "  [following] Processing 265 accounts followed by root\n",
      "\n",
      "[root] @larrabearr - fetching following...\n",
      "  Fetching following for user 46636556\n",
      "  [cache] Loading from following_46636556_page0.json\n",
      "    Found 173 accounts followed\n",
      "  [following] Processing 173 accounts followed by root\n",
      "\n",
      "[root] @LiangchenLuo - fetching following...\n",
      "  Fetching following for user 931209924385710080\n",
      "  [cache] Loading from following_931209924385710080_page0.json\n",
      "    Found 135 accounts followed\n",
      "  [following] Processing 135 accounts followed by root\n",
      "\n",
      "[root] @lm_zheng - fetching following...\n",
      "  Fetching following for user 952696452173594625\n",
      "  [cache] Loading from following_952696452173594625_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @mindbergh - fetching following...\n",
      "  Fetching following for user 1037380212\n",
      "  [cache] Loading from following_1037380212_page0.json\n",
      "    Found 208 accounts followed\n",
      "  [following] Processing 208 accounts followed by root\n",
      "\n",
      "[root] @MohitMittal26 - fetching following...\n",
      "  Fetching following for user 1992713924605825025\n",
      "  [cache] Loading from following_1992713924605825025_page0.json\n",
      "    Found 3 accounts followed\n",
      "  [following] Processing 3 accounts followed by root\n",
      "\n",
      "[root] @nmswede - fetching following...\n",
      "  Fetching following for user 74834469\n",
      "  [cache] Loading from following_74834469_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @Nitemaeric - fetching following...\n",
      "  Fetching following for user 275625421\n",
      "  [cache] Loading from following_275625421_page0.json\n",
      "    Found 177 accounts followed\n",
      "  [following] Processing 177 accounts followed by root\n",
      "\n",
      "[root] @onhachoe1 - fetching following...\n",
      "  Fetching following for user 55629516\n",
      "  [cache] Loading from following_55629516_page0.json\n",
      "    Found 43 accounts followed\n",
      "  [following] Processing 43 accounts followed by root\n",
      "\n",
      "[root] @pablo_pensees - fetching following...\n",
      "  Fetching following for user 2300690840\n",
      "  [cache] Loading from following_2300690840_page0.json\n",
      "    Found 235 accounts followed\n",
      "  [following] Processing 235 accounts followed by root\n",
      "\n",
      "[root] @pengshangfu - fetching following...\n",
      "  Fetching following for user 347198527\n",
      "  [cache] Loading from following_347198527_page0.json\n",
      "    Found 17 accounts followed\n",
      "  [following] Processing 17 accounts followed by root\n",
      "\n",
      "[root] @phroo - fetching following...\n",
      "  Fetching following for user 313672707\n",
      "  [cache] Loading from following_313672707_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @prolificgoober - fetching following...\n",
      "  Fetching following for user 2563553689\n",
      "  [cache] Loading from following_2563553689_page0.json\n",
      "    Found 59 accounts followed\n",
      "  [following] Processing 59 accounts followed by root\n",
      "\n",
      "[root] @rabahtahraoui - fetching following...\n",
      "  Fetching following for user 1120343086029209600\n",
      "  [cache] Loading from following_1120343086029209600_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @Randxie29 - fetching following...\n",
      "  Fetching following for user 767005845813026816\n",
      "  [cache] Loading from following_767005845813026816_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @rayhotate - fetching following...\n",
      "  Fetching following for user 917036353832984578\n",
      "  [cache] Loading from following_917036353832984578_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @riley_trett43 - fetching following...\n",
      "  Fetching following for user 1135283251\n",
      "  [cache] Loading from following_1135283251_page0.json\n",
      "    Found 254 accounts followed\n",
      "  [following] Processing 254 accounts followed by root\n",
      "\n",
      "[root] @rochan_hm - fetching following...\n",
      "  Fetching following for user 1280949979\n",
      "  [cache] Loading from following_1280949979_page0.json\n",
      "    Found 135 accounts followed\n",
      "  [following] Processing 135 accounts followed by root\n",
      "\n",
      "[root] @rpoo - fetching following...\n",
      "  Fetching following for user 314341900\n",
      "  [cache] Loading from following_314341900_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @RonghangHu - fetching following...\n",
      "  Fetching following for user 2757325333\n",
      "  [cache] Loading from following_2757325333_page0.json\n",
      "    Found 424 accounts followed\n",
      "  [following] Processing 424 accounts followed by root\n",
      "\n",
      "[root] @Saaaang94 - fetching following...\n",
      "  Fetching following for user 781024050848161796\n",
      "  [cache] Loading from following_781024050848161796_page0.json\n",
      "    Found 495 accounts followed\n",
      "  [following] Processing 495 accounts followed by root\n",
      "\n",
      "[root] @sahiljain314 - fetching following...\n",
      "  Fetching following for user 1099914793866035200\n",
      "  [cache] Loading from following_1099914793866035200_page0.json\n",
      "    Found 204 accounts followed\n",
      "  [following] Processing 204 accounts followed by root\n",
      "\n",
      "[root] @satu0king - fetching following...\n",
      "  Fetching following for user 1356657338\n",
      "  [cache] Loading from following_1356657338_page0.json\n",
      "    Found 20 accounts followed\n",
      "  [following] Processing 20 accounts followed by root\n",
      "\n",
      "[root] @shengs1123 - fetching following...\n",
      "  Fetching following for user 1103577756909023232\n",
      "  [cache] Loading from following_1103577756909023232_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @shefys - fetching following...\n",
      "  Fetching following for user 11451052\n",
      "  [cache] Loading from following_11451052_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @sishaar - fetching following...\n",
      "  Fetching following for user 578176008\n",
      "  [cache] Loading from following_578176008_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @skory_by - fetching following...\n",
      "  Fetching following for user 1394449160\n",
      "  [cache] Loading from following_1394449160_page0.json\n",
      "    Found 220 accounts followed\n",
      "  [following] Processing 220 accounts followed by root\n",
      "\n",
      "[root] @spencersharkey - fetching following...\n",
      "  Fetching following for user 2345951964\n",
      "  [cache] Loading from following_2345951964_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @stepango - fetching following...\n",
      "  Fetching following for user 70057560\n",
      "  [cache] Loading from following_70057560_page0.json\n",
      "    Found 435 accounts followed\n",
      "  [following] Processing 435 accounts followed by root\n",
      "\n",
      "[root] @StewartSlocum1 - fetching following...\n",
      "  Fetching following for user 1172647422587064322\n",
      "  [cache] Loading from following_1172647422587064322_page0.json\n",
      "    Found 161 accounts followed\n",
      "  [following] Processing 161 accounts followed by root\n",
      "\n",
      "[root] @textangel - fetching following...\n",
      "  Fetching following for user 105961577\n",
      "  [cache] Loading from following_105961577_page0.json\n",
      "    Found 499 accounts followed\n",
      "  [following] Processing 499 accounts followed by root\n",
      "\n",
      "[root] @thebigreesh - fetching following...\n",
      "  Fetching following for user 2439038563\n",
      "  [cache] Loading from following_2439038563_page0.json\n",
      "    Found 38 accounts followed\n",
      "  [following] Processing 38 accounts followed by root\n",
      "\n",
      "[root] @ThePenguinCo - fetching following...\n",
      "  Fetching following for user 2427750102\n",
      "  [cache] Loading from following_2427750102_page0.json\n",
      "    Found 153 accounts followed\n",
      "  [following] Processing 153 accounts followed by root\n",
      "\n",
      "[root] @TobyPhln - fetching following...\n",
      "  Fetching following for user 1023014000\n",
      "  [cache] Loading from following_1023014000_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @tylerkniess - fetching following...\n",
      "  Fetching following for user 80460759\n",
      "  [cache] Loading from following_80460759_page0.json\n",
      "    Found 484 accounts followed\n",
      "  [following] Processing 484 accounts followed by root\n",
      "\n",
      "[root] @tylervstorm - fetching following...\n",
      "  Fetching following for user 899720210084765696\n",
      "  [cache] Loading from following_899720210084765696_page0.json\n",
      "    Found 314 accounts followed\n",
      "  [following] Processing 314 accounts followed by root\n",
      "\n",
      "[root] @WillFuxAI - fetching following...\n",
      "  Fetching following for user 1997353872688635904\n",
      "  [cache] Loading from following_1997353872688635904_page0.json\n",
      "    Found 1 accounts followed\n",
      "  [following] Processing 1 accounts followed by root\n",
      "\n",
      "[root] @wyudun - fetching following...\n",
      "  Fetching following for user 2449279502\n",
      "  [cache] Loading from following_2449279502_page0.json\n",
      "    Found 165 accounts followed\n",
      "  [following] Processing 165 accounts followed by root\n",
      "\n",
      "[root] @x_hanyu - fetching following...\n",
      "  Fetching following for user 2764212415\n",
      "  [cache] Loading from following_2764212415_page0.json\n",
      "    Found 116 accounts followed\n",
      "  [following] Processing 116 accounts followed by root\n",
      "\n",
      "[root] @xiaobinwu - fetching following...\n",
      "  Fetching following for user 99208056\n",
      "  [cache] Loading from following_99208056_page0.json\n",
      "    Found 256 accounts followed\n",
      "  [following] Processing 256 accounts followed by root\n",
      "\n",
      "[root] @Yang1fan2 - fetching following...\n",
      "  Fetching following for user 332658158\n",
      "  [cache] Loading from following_332658158_page0.json\n",
      "    Found 349 accounts followed\n",
      "  [following] Processing 349 accounts followed by root\n",
      "\n",
      "[root] @YangNaruto - fetching following...\n",
      "  Fetching following for user 912642665870774273\n",
      "  [cache] Loading from following_912642665870774273_page0.json\n",
      "    Found 92 accounts followed\n",
      "  [following] Processing 92 accounts followed by root\n",
      "\n",
      "[root] @yiwenyuan98 - fetching following...\n",
      "  Fetching following for user 1093044471447646209\n",
      "  [cache] Loading from following_1093044471447646209_page0.json\n",
      "    Found 101 accounts followed\n",
      "  [following] Processing 101 accounts followed by root\n",
      "\n",
      "[root] @zhangir_azerbay - fetching following...\n",
      "  Fetching following for user 1148267279718801408\n",
      "  [cache] Loading from following_1148267279718801408_page0.json\n",
      "    Found 500 accounts followed\n",
      "  [following] Processing 500 accounts followed by root\n",
      "\n",
      "[root] @zheweishi - fetching following...\n",
      "  Fetching following for user 2734470248\n",
      "  [cache] Loading from following_2734470248_page0.json\n",
      "    Found 80 accounts followed\n",
      "  [following] Processing 80 accounts followed by root\n",
      "\n",
      "Nodes so far: 15748\n",
      "Edges so far: 27266\n"
     ]
    }
   ],
   "source": [
    "# Phase 2a: Expand via FOLLOWING (strongest signal)\n",
    "# Processing ALL seeds (not just first 5)\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2a: Expanding via FOLLOWING (strongest signal)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for handle, root_user in root_users.items():  # ALL seeds\n",
    "    root_id = root_user[\"id\"]\n",
    "    print(f\"\\n[root] @{handle} - fetching following...\")\n",
    "    builder._expand_following(root_id)\n",
    "\n",
    "print(f\"\\nNodes so far: {len(builder.nodes)}\")\n",
    "print(f\"Edges so far: {len(builder.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Phase 2c: Expand via ROOT TWEETS (retweets, replies)\n",
    "# # Processing ALL seeds\n",
    "# print(\"=\" * 60)\n",
    "# print(\"PHASE 2c: Expanding via ROOT TWEETS (retweets, replies)\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# for handle, root_user in root_users.items():  # ALL seeds\n",
    "#     root_id = root_user[\"id\"]\n",
    "#     print(f\"\\n[root] @{handle} - fetching tweet engagements...\")\n",
    "#     builder._expand_root_tweets(root_id)\n",
    "\n",
    "# print(f\"\\nNodes so far: {len(builder.nodes)}\")\n",
    "# print(f\"Edges so far: {len(builder.edges)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2c: Expand via ROOT TWEETS (retweets, replies)\n",
    "# This phase is OPTIONAL - following data (Phase 2a) is the strongest signal\n",
    "# Skip this if hitting rate limits\n",
    "\n",
    "ENABLE_PHASE_2C = False  # Set to True to enable (will hit rate limits with 90 seeds)\n",
    "\n",
    "if ENABLE_PHASE_2C:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PHASE 2c: Expanding via ROOT TWEETS (retweets, replies)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for handle, root_user in root_users.items():\n",
    "        root_id = root_user[\"id\"]\n",
    "        print(f\"\\n[root] @{handle} - fetching tweet engagements...\")\n",
    "        builder._expand_root_tweets(root_id)\n",
    "    \n",
    "    print(f\"\\nNodes so far: {len(builder.nodes)}\")\n",
    "    print(f\"Edges so far: {len(builder.edges)}\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PHASE 2c: SKIPPED (rate limit protection)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  [info] Following data from Phase 2a is the strongest signal\")\n",
    "    print(\"  [info] Set ENABLE_PHASE_2C = True to enable retweet/reply expansion\")\n",
    "    print(f\"\\nNodes so far: {len(builder.nodes)}\")\n",
    "    print(f\"Edges so far: {len(builder.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 3: Hydrating discovered users\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "DISCOVERY COMPLETE (Phase 1-3)\n",
      "============================================================\n",
      "  Total nodes: 15,748\n",
      "  Total edges: 27,266\n",
      "  Root accounts: 89\n",
      "  Candidate accounts: 10,694\n",
      "  Edge breakdown: {'follow': 27266}\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Hydrate pending users\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 3: Hydrating discovered users\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "builder._hydrate_pending_users()\n",
    "\n",
    "# Count results\n",
    "total_nodes = len(builder.nodes)\n",
    "total_edges = len(builder.edges)\n",
    "root_count = sum(1 for n in builder.nodes.values() if n.is_root)\n",
    "candidate_count = sum(1 for n in builder.nodes.values() if n.is_candidate)\n",
    "\n",
    "# Edge breakdown\n",
    "edge_counts = {}\n",
    "for edge in builder.edges:\n",
    "    edge_counts[edge.interaction_type] = edge_counts.get(edge.interaction_type, 0) + 1\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"DISCOVERY COMPLETE (Phase 1-3)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total nodes: {total_nodes:,}\")\n",
    "print(f\"  Total edges: {total_edges:,}\")\n",
    "print(f\"  Root accounts: {root_count}\")\n",
    "print(f\"  Candidate accounts: {candidate_count:,}\")\n",
    "print(f\"  Edge breakdown: {edge_counts}\")\n",
    "\n",
    "# Show filtered xAI/X employees\n",
    "if hasattr(builder, 'filtered_employees') and builder.filtered_employees:\n",
    "    print(f\"\\n  [FILTERED] {len(builder.filtered_employees)} xAI/X employees excluded:\")\n",
    "    for handle in builder.filtered_employees[:20]:\n",
    "        print(f\"    - @{handle}\")\n",
    "    if len(builder.filtered_employees) > 20:\n",
    "        print(f\"    ... and {len(builder.filtered_employees) - 20} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3b: Multi-depth expansion (SKIP - use cell below instead)\n",
    "\n",
    "The cell below (cell-14) has rate-limit aware settings. Run that one instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MULTI-DEPTH EXPANSION (up to depth 5)\n",
      "============================================================\n",
      "  Config: top_k=15, max_following=50\n",
      "\n",
      "========================================\n",
      "DEPTH 2\n",
      "========================================\n",
      "  Expanding from top 15 depth-1 candidates\n",
      "    @jimmybajimmyba (score: 48)...   Fetching following for user 1588323862110162944\n",
      "  [cache] Loading from following_1588323862110162944_page0.json\n",
      "    Found 100 accounts followed\n",
      "+11 new\n",
      "    @ericzelikman (score: 43)...   Fetching following for user 137463715\n",
      "  [cache] Loading from following_137463715_page0.json\n",
      "    Found 100 accounts followed\n",
      "+32 new\n",
      "    @Yuhu_ai_ (score: 42)...   Fetching following for user 881959726958862337\n",
      "  [cache] Loading from following_881959726958862337_page0.json\n",
      "    Found 100 accounts followed\n",
      "+3 new\n",
      "    @HeinrichKuttler (score: 38)...   Fetching following for user 1260917258970451969\n",
      "  [cache] Loading from following_1260917258970451969_page0.json\n",
      "    Found 100 accounts followed\n",
      "+19 new\n",
      "    @veggie_eric (score: 37)...   Fetching following for user 1219282049070063617\n",
      "  [cache] Loading from following_1219282049070063617_page0.json\n",
      "    Found 100 accounts followed\n",
      "+15 new\n",
      "    @LiLiDuc22 (score: 36)...   Fetching following for user 1838355640890331137\n",
      "  [cache] Loading from following_1838355640890331137_page0.json\n",
      "    Found 100 accounts followed\n",
      "+24 new\n",
      "    @512x512 (score: 35)...   Fetching following for user 1058927125\n",
      "  [cache] Loading from following_1058927125_page0.json\n",
      "    Found 100 accounts followed\n",
      "+19 new\n",
      "    @EthanHe_42 (score: 34)...   Fetching following for user 3086182267\n",
      "  [cache] Loading from following_3086182267_page0.json\n",
      "    Found 100 accounts followed\n",
      "+22 new\n",
      "    @Diegopasini (score: 31)...   Fetching following for user 1648128136997904384\n",
      "  [cache] Loading from following_1648128136997904384_page0.json\n",
      "    Found 100 accounts followed\n",
      "+7 new\n",
      "    @skcd42 (score: 30)...   Fetching following for user 1480673713179795457\n",
      "  [cache] Loading from following_1480673713179795457_page0.json\n",
      "    Found 100 accounts followed\n",
      "+6 new\n",
      "    @ebbyamir (score: 30)...   Fetching following for user 423216326\n",
      "  [cache] Loading from following_423216326_page0.json\n",
      "    Found 100 accounts followed\n",
      "+22 new\n",
      "    @belce_dogru (score: 29)...   Fetching following for user 834832429709828096\n",
      "  [cache] Loading from following_834832429709828096_page0.json\n",
      "    Found 100 accounts followed\n",
      "+10 new\n",
      "    @VastoLorde95 (score: 29)...   Fetching following for user 1572675547\n",
      "  [cache] Loading from following_1572675547_page0.json\n",
      "    Found 100 accounts followed\n",
      "+14 new\n",
      "    @MohitReddy13 (score: 29)...   Fetching following for user 863617319783870464\n",
      "  [cache] Loading from following_863617319783870464_page0.json\n",
      "    Found 100 accounts followed\n",
      "+11 new\n",
      "    @YuchenHe07 (score: 28)...   Fetching following for user 71810803\n",
      "  [cache] Loading from following_71810803_page0.json\n",
      "    Found 100 accounts followed\n",
      "+23 new\n",
      "\n",
      "  Total discovered at depth 2: 238\n",
      "\n",
      "========================================\n",
      "DEPTH 3\n",
      "========================================\n",
      "  Expanding from top 15 depth-2 candidates\n",
      "    @juleszqiu (score: 1)...   Fetching following for user 1654603020192333827\n",
      "  [cache] Loading from following_1654603020192333827_page0.json\n",
      "    Found 42 accounts followed\n",
      "+31 new\n",
      "    @jamescham (score: 1)...   Fetching following for user 9316032\n",
      "  [cache] Loading from following_9316032_page0.json\n",
      "    Found 100 accounts followed\n",
      "+36 new\n",
      "    @goldenkatepark (score: 1)...   Fetching following for user 3021662641\n",
      "  [cache] Loading from following_3021662641_page0.json\n",
      "    Found 100 accounts followed\n",
      "+9 new\n",
      "    @AlphaZu_ (score: 1)...   Fetching following for user 1691544720617639936\n",
      "  [cache] Loading from following_1691544720617639936_page0.json\n",
      "    Found 100 accounts followed\n",
      "+21 new\n",
      "    @wchan212 (score: 1)...   Fetching following for user 497810305\n",
      "  [cache] Loading from following_497810305_page0.json\n",
      "    Found 100 accounts followed\n",
      "+15 new\n",
      "    @amandamhuang (score: 1)...   Fetching following for user 1562706964026576897\n",
      "  [cache] Loading from following_1562706964026576897_page0.json\n",
      "    Found 100 accounts followed\n",
      "+30 new\n",
      "    @jonoringer (score: 1)...   Fetching following for user 23890475\n",
      "  [cache] Loading from following_23890475_page0.json\n",
      "    Found 100 accounts followed\n",
      "+26 new\n",
      "    @echen (score: 1)...   Fetching following for user 67653490\n",
      "  [cache] Loading from following_67653490_page0.json\n",
      "    Found 100 accounts followed\n",
      "+25 new\n",
      "    @jingli9111 (score: 1)...   Fetching following for user 1476149899\n",
      "  [cache] Loading from following_1476149899_page0.json\n",
      "    Found 100 accounts followed\n",
      "+10 new\n",
      "    @jimhurd (score: 1)...   Fetching following for user 15133995\n",
      "  [cache] Loading from following_15133995_page0.json\n",
      "    Found 100 accounts followed\n",
      "+41 new\n",
      "    @solsticestone (score: 1)...   Fetching following for user 406104936\n",
      "  [cache] Loading from following_406104936_page0.json\n",
      "    Found 100 accounts followed\n",
      "+27 new\n",
      "    @BillZheng155508 (score: 1)...   Fetching following for user 1676035342879834112\n",
      "  [cache] Loading from following_1676035342879834112_page0.json\n",
      "    Found 100 accounts followed\n",
      "+32 new\n",
      "    @realChrisChoy (score: 1)...   Fetching following for user 3317678802\n",
      "  [cache] Loading from following_3317678802_page0.json\n",
      "    Found 100 accounts followed\n",
      "+25 new\n",
      "    @Xinyu2ML (score: 1)...   Fetching following for user 1601134489161400321\n",
      "  [cache] Loading from following_1601134489161400321_page0.json\n",
      "    Found 100 accounts followed\n",
      "+32 new\n",
      "    @jaewon_chung_cs (score: 1)...   Fetching following for user 1370697123365199874\n",
      "  [cache] Loading from following_1370697123365199874_page0.json\n",
      "    Found 100 accounts followed\n",
      "+23 new\n",
      "\n",
      "  Total discovered at depth 3: 383\n",
      "\n",
      "========================================\n",
      "DEPTH 4\n",
      "========================================\n",
      "  Expanding from top 15 depth-3 candidates\n",
      "    @soohoonchoi (score: 1)...   Fetching following for user 489186333\n",
      "  [cache] Loading from following_489186333_page0.json\n",
      "    Found 100 accounts followed\n",
      "+26 new\n",
      "    @henry_yu_01 (score: 1)...   Fetching following for user 1985496909575073792\n",
      "  [cache] Loading from following_1985496909575073792_page0.json\n",
      "    Found 100 accounts followed\n",
      "+34 new\n",
      "    @Rahul_Venkatesh (score: 1)...   Fetching following for user 88371693\n",
      "  [cache] Loading from following_88371693_page0.json\n",
      "    Found 100 accounts followed\n",
      "+37 new\n",
      "    @JoyHeYueya (score: 1)...   Fetching following for user 4054310807\n",
      "  [cache] Loading from following_4054310807_page0.json\n",
      "    Found 96 accounts followed\n",
      "+27 new\n",
      "    @minafahmi (score: 1)...   Fetching following for user 1289980444134514710\n",
      "  [cache] Loading from following_1289980444134514710_page0.json\n",
      "    Found 50 accounts followed\n",
      "+32 new\n",
      "    @getcompoundai (score: 1)...   Fetching following for user 1968004282449584128\n",
      "  [cache] Loading from following_1968004282449584128_page0.json\n",
      "    Found 0 accounts followed\n",
      "+0 new\n",
      "    @austinywang (score: 1)...   Fetching following for user 1423529493264666624\n",
      "  [cache] Loading from following_1423529493264666624_page0.json\n",
      "    Found 100 accounts followed\n",
      "+38 new\n",
      "    @EmilVLiu (score: 1)...   Fetching following for user 1154566971045924869\n",
      "  [cache] Loading from following_1154566971045924869_page0.json\n",
      "    Found 100 accounts followed\n",
      "+26 new\n",
      "    @adelwu_ (score: 1)...   Fetching following for user 1475703441032990723\n",
      "  [cache] Loading from following_1475703441032990723_page0.json\n",
      "    Found 100 accounts followed\n",
      "+41 new\n",
      "    @sanjana__z (score: 1)...   Fetching following for user 1235812280895328262\n",
      "  [cache] Loading from following_1235812280895328262_page0.json\n",
      "    Found 100 accounts followed\n",
      "+25 new\n",
      "    @HarvardMath (score: 1)...   Fetching following for user 1654028330\n",
      "  [cache] Loading from following_1654028330_page0.json\n",
      "    Found 63 accounts followed\n",
      "+42 new\n",
      "    @Harvard_History (score: 1)...   Fetching following for user 632991157\n",
      "  [cache] Loading from following_632991157_page0.json\n",
      "    Found 100 accounts followed\n",
      "+50 new\n",
      "    @KyleSargentAI (score: 1)...   Fetching following for user 971242879\n",
      "  [cache] Loading from following_971242879_page0.json\n",
      "    Found 100 accounts followed\n",
      "+28 new\n",
      "    @j_mcgraph (score: 1)...   Fetching following for user 969439214\n",
      "  [cache] Loading from following_969439214_page0.json\n",
      "    Found 100 accounts followed\n",
      "+26 new\n",
      "    @_RobToews (score: 1)...   Fetching following for user 848117533\n",
      "  [cache] Loading from following_848117533_page0.json\n",
      "    Found 100 accounts followed\n",
      "+20 new\n",
      "\n",
      "  Total discovered at depth 4: 452\n",
      "\n",
      "========================================\n",
      "DEPTH 5\n",
      "========================================\n",
      "  Expanding from top 15 depth-4 candidates\n",
      "    @io_sammt (score: 1)...   Fetching following for user 554095426\n",
      "  [cache] Loading from following_554095426_page0.json\n",
      "    Found 100 accounts followed\n",
      "+30 new\n",
      "    @lutherlowe (score: 1)...   Fetching following for user 9864482\n",
      "  [cache] Loading from following_9864482_page0.json\n",
      "    Found 100 accounts followed\n",
      "+37 new\n",
      "    @ksw_arman (score: 1)...   Fetching following for user 1744070628733317121\n",
      "  [cache] Loading from following_1744070628733317121_page0.json\n",
      "    Found 100 accounts followed\n",
      "+42 new\n",
      "    @CharlieEriksen (score: 1)...   Fetching following for user 18391186\n",
      "  [cache] Loading from following_18391186_page0.json\n",
      "    Found 100 accounts followed\n",
      "+43 new\n",
      "    @lesswrong (score: 1)...   Fetching following for user 1172582121929465856\n",
      "  [cache] Loading from following_1172582121929465856_page0.json\n",
      "    Found 5 accounts followed\n",
      "+0 new\n",
      "    @jessi_cata (score: 1)...   Fetching following for user 361354984\n",
      "  [cache] Loading from following_361354984_page0.json\n",
      "    Found 100 accounts followed\n",
      "+34 new\n",
      "    @Steve_Yegge (score: 1)...   Fetching following for user 429567341\n",
      "  [cache] Loading from following_429567341_page0.json\n",
      "    Found 12 accounts followed\n",
      "+5 new\n",
      "    @rankintweets (score: 1)...   Fetching following for user 1483580981441449984\n",
      "  [cache] Loading from following_1483580981441449984_page0.json\n",
      "    Found 100 accounts followed\n",
      "+38 new\n",
      "    @traestephens (score: 1)...   Fetching following for user 131307382\n",
      "  [cache] Loading from following_131307382_page0.json\n",
      "    Found 100 accounts followed\n",
      "+32 new\n",
      "    @dotkrish (score: 1)...   Fetching following for user 1463175795380146176\n",
      "  [cache] Loading from following_1463175795380146176_page0.json\n",
      "    Found 100 accounts followed\n",
      "+39 new\n",
      "    @barre_of_lube (score: 1)...   Fetching following for user 1895143864651927552\n",
      "  [cache] Loading from following_1895143864651927552_page0.json\n",
      "    Found 100 accounts followed\n",
      "+37 new\n",
      "    @agithief (score: 1)...   Fetching following for user 1927879217641029632\n",
      "  [cache] Loading from following_1927879217641029632_page0.json\n",
      "    Found 100 accounts followed\n",
      "+35 new\n",
      "    @CathTeto (score: 1)...   Fetching following for user 1892847335761219584\n",
      "  [cache] Loading from following_1892847335761219584_page0.json\n",
      "    Found 100 accounts followed\n",
      "+48 new\n",
      "    @alec_helbling (score: 1)...   Fetching following for user 940695781211811841\n",
      "  [cache] Loading from following_940695781211811841_page0.json\n",
      "    Found 100 accounts followed\n",
      "+34 new\n",
      "    @No5mallf3at (score: 1)...   Fetching following for user 1814250479117012993\n",
      "  [cache] Loading from following_1814250479117012993_page0.json\n",
      "    Found 100 accounts followed\n",
      "+48 new\n",
      "\n",
      "  Total discovered at depth 5: 502\n",
      "\n",
      "============================================================\n",
      "MULTI-DEPTH EXPANSION COMPLETE\n",
      "============================================================\n",
      "  Total nodes: 17,323\n",
      "  Total edges: 28,841\n",
      "\n",
      "Candidate breakdown by depth:\n",
      "  depth_1: 15,659\n",
      "  depth_2: 238\n",
      "  depth_3: 383\n",
      "  depth_4: 452\n",
      "  depth_5: 502\n"
     ]
    }
   ],
   "source": [
    "# Phase 3b: Multi-depth expansion (up to depth 5)\n",
    "# RATE LIMIT AWARE: Reduced parameters to stay under API limits\n",
    "\n",
    "MAX_DEPTH = 5           # Go up to 5 hops from seeds\n",
    "TOP_K_PER_DEPTH = 15    # Expand from top 15 candidates at each depth\n",
    "MAX_FOLLOWING = 50      # Fetch up to 50 accounts each follows\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"MULTI-DEPTH EXPANSION (up to depth {MAX_DEPTH})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Config: top_k={TOP_K_PER_DEPTH}, max_following={MAX_FOLLOWING}\")\n",
    "\n",
    "for depth in range(2, MAX_DEPTH + 1):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"DEPTH {depth}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    # Find candidates from previous depth with highest in-degree\n",
    "    candidate_scores = {}\n",
    "    \n",
    "    for edge in builder.edges:\n",
    "        if edge.interaction_type == \"follow\" and edge.depth == depth - 1:\n",
    "            dst = edge.dst_user_id\n",
    "            if dst in builder.nodes and builder.nodes[dst].is_candidate:\n",
    "                candidate_scores[dst] = candidate_scores.get(dst, 0) + 1\n",
    "    \n",
    "    if not candidate_scores:\n",
    "        print(f\"  No candidates with incoming edges at depth {depth-1}, stopping expansion\")\n",
    "        break\n",
    "    \n",
    "    # Sort by score and take top k\n",
    "    top_candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)[:TOP_K_PER_DEPTH]\n",
    "    \n",
    "    print(f\"  Expanding from top {len(top_candidates)} depth-{depth-1} candidates\")\n",
    "    \n",
    "    new_discoveries = 0\n",
    "    \n",
    "    for user_id, score in top_candidates:\n",
    "        node = builder.nodes.get(user_id)\n",
    "        if not node:\n",
    "            continue\n",
    "        \n",
    "        print(f\"    @{node.handle} (score: {score})...\", end=\" \")\n",
    "        \n",
    "        # Get who this candidate follows\n",
    "        following = builder.x.get_user_following(user_id, max_results=MAX_FOLLOWING)\n",
    "        \n",
    "        added = 0\n",
    "        for user in following:\n",
    "            fol_id = user.get(\"id\")\n",
    "            if not fol_id or fol_id == user_id:\n",
    "                continue\n",
    "            \n",
    "            # Skip if already in graph\n",
    "            if fol_id in builder.nodes:\n",
    "                continue\n",
    "            \n",
    "            # Add new node\n",
    "            new_node = builder._add_or_update_node(user, discovered_via=f\"depth{depth}_following\")\n",
    "            if new_node:\n",
    "                new_discoveries += 1\n",
    "                added += 1\n",
    "                \n",
    "                # Add edge with current depth\n",
    "                builder._add_edge(\n",
    "                    src_user_id=user_id,\n",
    "                    dst_user_id=fol_id,\n",
    "                    interaction_type=\"follow\",\n",
    "                    tweet_id=\"\",\n",
    "                    created_at=\"\",\n",
    "                    depth=depth,\n",
    "                )\n",
    "        \n",
    "        print(f\"+{added} new\")\n",
    "    \n",
    "    print(f\"\\n  Total discovered at depth {depth}: {new_discoveries}\")\n",
    "    \n",
    "    # Hydrate new users\n",
    "    if new_discoveries > 0:\n",
    "        builder._hydrate_pending_users()\n",
    "    else:\n",
    "        print(f\"  No new discoveries at depth {depth}, stopping expansion\")\n",
    "        break\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"MULTI-DEPTH EXPANSION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Total nodes: {len(builder.nodes):,}\")\n",
    "print(f\"  Total edges: {len(builder.edges):,}\")\n",
    "\n",
    "# Count by depth\n",
    "depth_counts = {}\n",
    "for node in builder.nodes.values():\n",
    "    via = node.discovered_via\n",
    "    if via.startswith(\"depth\") and via.endswith(\"_following\"):\n",
    "        d = via.replace(\"depth\", \"\").replace(\"_following\", \"\")\n",
    "        depth_counts[f\"depth_{d}\"] = depth_counts.get(f\"depth_{d}\", 0) + 1\n",
    "    elif via == \"followed_by_root\":\n",
    "        depth_counts[\"depth_1\"] = depth_counts.get(\"depth_1\", 0) + 1\n",
    "    elif via in [\"retweeted_root\", \"replied_to_root\"]:\n",
    "        depth_counts[\"depth_1_engagement\"] = depth_counts.get(\"depth_1_engagement\", 0) + 1\n",
    "\n",
    "print(\"\\nCandidate breakdown by depth:\")\n",
    "for d, count in sorted(depth_counts.items()):\n",
    "    print(f\"  {d}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 12,050 candidates for evaluation\n",
      "\n",
      "Discovery method breakdown:\n",
      "  followed_by_root: 10,694\n",
      "  depth2_following: 204\n",
      "  depth3_following: 314\n",
      "  depth4_following: 403\n",
      "  depth5_following: 435\n"
     ]
    }
   ],
   "source": [
    "# Prepare candidates for evaluation\n",
    "# Priority: followed_by_root > liked_by_root > retweeted_root > replied_to_root\n",
    "discovery_priority = {\n",
    "    \"followed_by_root\": 0,\n",
    "    \"liked_by_root\": 1,\n",
    "    \"retweeted_root\": 2,\n",
    "    \"replied_to_root\": 3,\n",
    "    \"liked_root_tweet\": 4,\n",
    "}\n",
    "\n",
    "candidates = [\n",
    "    {\n",
    "        \"user_id\": node.user_id,\n",
    "        \"handle\": node.handle,\n",
    "        \"bio\": node.bio,\n",
    "        \"followers_count\": node.followers_count,\n",
    "        \"tweet_count\": node.tweet_count,\n",
    "        \"discovered_via\": node.discovered_via,\n",
    "    }\n",
    "    for node in builder.nodes.values()\n",
    "    if node.is_candidate\n",
    "]\n",
    "\n",
    "# Sort by discovery priority\n",
    "candidates.sort(key=lambda c: discovery_priority.get(c.get(\"discovered_via\", \"\"), 99))\n",
    "\n",
    "print(f\"Prepared {len(candidates):,} candidates for evaluation\")\n",
    "print(f\"\\nDiscovery method breakdown:\")\n",
    "discovery_counts = {}\n",
    "for c in candidates:\n",
    "    via = c.get(\"discovered_via\", \"unknown\")\n",
    "    discovery_counts[via] = discovery_counts.get(via, 0) + 1\n",
    "for via, count in sorted(discovery_counts.items(), key=lambda x: discovery_priority.get(x[0], 99)):\n",
    "    print(f\"  {via}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph visualization function ready!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def visualize_graph(builder: GraphBuilder, max_nodes: int = 500, interaction_filter: list = None, \n",
    "                    layout: str = \"spring\", show_labels: bool = False):\n",
    "    '''\n",
    "    Visualize the graph showing different connection types.\n",
    "    \n",
    "    Args:\n",
    "        builder: GraphBuilder instance with nodes and edges\n",
    "        max_nodes: Maximum number of nodes to display (for performance)\n",
    "        interaction_filter: List of interaction types to show (e.g., ['retweet', 'reply'])\n",
    "                          If None, shows all types\n",
    "        layout: Layout algorithm ('spring', 'kamada_kawai', 'circular')\n",
    "        show_labels: Whether to show node labels (can be slow for large graphs)\n",
    "    '''\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    edge_type_colors = {\n",
    "        'follow': '#1DA1F2',    # Twitter blue\n",
    "        'retweet': '#17BF63',   # Green \n",
    "        'reply': '#FFAD1F',     # Orange\n",
    "        'like': '#E0245E',      # Red/pink\n",
    "        'quote': '#794BC4'      # Purple\n",
    "    }\n",
    "    \n",
    "    edge_type_widths = {\n",
    "        'follow': 2.0,\n",
    "        'retweet': 1.5,\n",
    "        'reply': 1.2,\n",
    "        'like': 0.8,\n",
    "        'quote': 1.0\n",
    "    }\n",
    "    \n",
    "    root_ids = {n.user_id for n in builder.nodes.values() if n.is_root}\n",
    "    \n",
    "    # Build full graph first\n",
    "    for edge in builder.edges:\n",
    "        if interaction_filter and edge.interaction_type not in interaction_filter:\n",
    "            continue\n",
    "        G.add_edge(edge.src_user_id, edge.dst_user_id, \n",
    "                   interaction_type=edge.interaction_type, weight=edge.weight)\n",
    "    \n",
    "    # Add node attributes\n",
    "    for node_id in G.nodes():\n",
    "        if node_id in builder.nodes:\n",
    "            node = builder.nodes[node_id]\n",
    "            G.nodes[node_id]['handle'] = node.handle\n",
    "            G.nodes[node_id]['is_root'] = node.is_root\n",
    "            G.nodes[node_id]['is_candidate'] = node.is_candidate\n",
    "            G.nodes[node_id]['followers'] = node.followers_count\n",
    "    \n",
    "    print(f\"Full graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # If too large, sample while preserving connectivity\n",
    "    if G.number_of_nodes() > max_nodes:\n",
    "        print(f\"Sampling {max_nodes} most connected nodes (keeping all roots)...\")\n",
    "        \n",
    "        # Always keep roots\n",
    "        nodes_to_keep = set(root_ids & set(G.nodes()))\n",
    "        \n",
    "        # Get nodes by degree (most connected first)\n",
    "        node_degrees = dict(G.degree())\n",
    "        sorted_nodes = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Add highest-degree nodes until we hit max\n",
    "        for node_id, degree in sorted_nodes:\n",
    "            if len(nodes_to_keep) >= max_nodes:\n",
    "                break\n",
    "            nodes_to_keep.add(node_id)\n",
    "        \n",
    "        # Create subgraph\n",
    "        G = G.subgraph(nodes_to_keep).copy()\n",
    "        print(f\"Sampled graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Count edges by type\n",
    "    edge_counts_by_type = Counter()\n",
    "    for _, _, d in G.edges(data=True):\n",
    "        edge_counts_by_type[d.get('interaction_type', 'unknown')] += 1\n",
    "    \n",
    "    print(f\"Edge breakdown: {dict(edge_counts_by_type)}\")\n",
    "    \n",
    "    # Choose layout\n",
    "    print(f\"Computing {layout} layout...\")\n",
    "    if layout == \"kamada_kawai\":\n",
    "        pos = nx.kamada_kawai_layout(G)\n",
    "    elif layout == \"circular\":\n",
    "        pos = nx.circular_layout(G)\n",
    "    else:  # spring\n",
    "        pos = nx.spring_layout(G, k=1.5/len(G.nodes())**0.5, iterations=50, seed=42)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(16, 12))\n",
    "    \n",
    "    # Categorize nodes\n",
    "    root_nodes = [n for n in G.nodes() if G.nodes[n].get('is_root', False)]\n",
    "    candidate_nodes = [n for n in G.nodes() if G.nodes[n].get('is_candidate', False) and n not in root_nodes]\n",
    "    other_nodes = [n for n in G.nodes() if n not in root_nodes and n not in candidate_nodes]\n",
    "    \n",
    "    # Draw edges by type (bottom layer)\n",
    "    for interaction_type in ['like', 'reply', 'retweet', 'follow', 'quote']:\n",
    "        edges_of_type = [(u, v) for u, v, d in G.edges(data=True) \n",
    "                         if d.get('interaction_type') == interaction_type]\n",
    "        if edges_of_type:\n",
    "            nx.draw_networkx_edges(\n",
    "                G, pos, edgelist=edges_of_type, ax=ax,\n",
    "                edge_color=edge_type_colors.get(interaction_type, '#CCCCCC'),\n",
    "                width=edge_type_widths.get(interaction_type, 1.0),\n",
    "                alpha=0.3,\n",
    "                arrowsize=8,\n",
    "                arrowstyle='-|>',\n",
    "                connectionstyle='arc3,rad=0.1'\n",
    "            )\n",
    "    \n",
    "    # Draw nodes (top layer)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=other_nodes, node_color='#B8D4E3', \n",
    "                          node_size=30, alpha=0.5, ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=candidate_nodes, node_color='#4ECDC4', \n",
    "                          node_size=80, alpha=0.7, ax=ax)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=root_nodes, node_color='#FF6B6B', \n",
    "                          node_size=200, alpha=0.9, ax=ax)\n",
    "    \n",
    "    # Draw labels for roots\n",
    "    if show_labels or len(root_nodes) <= 20:\n",
    "        root_labels = {n: G.nodes[n].get('handle', '')[:12] for n in root_nodes}\n",
    "        nx.draw_networkx_labels(G, pos, root_labels, font_size=7, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Create legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#FF6B6B', label=f'Seeds ({len(root_nodes)})'),\n",
    "        Patch(facecolor='#4ECDC4', label=f'Candidates ({len(candidate_nodes)})'),\n",
    "        Patch(facecolor='#B8D4E3', label=f'Other ({len(other_nodes)})'),\n",
    "        Line2D([0], [0], color='white', label=''),  # Spacer\n",
    "    ]\n",
    "    \n",
    "    # Add edge type legends\n",
    "    for itype in ['follow', 'retweet', 'reply', 'like', 'quote']:\n",
    "        if edge_counts_by_type.get(itype, 0) > 0:\n",
    "            legend_elements.append(\n",
    "                Line2D([0], [0], color=edge_type_colors[itype], \n",
    "                       linewidth=edge_type_widths[itype], \n",
    "                       label=f'{itype.capitalize()} ({edge_counts_by_type[itype]:,})')\n",
    "            )\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=9, framealpha=0.9)\n",
    "    \n",
    "    ax.set_title(f'Taste Graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/processed/graph_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Saved to data/processed/graph_visualization.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return G\n",
    "\n",
    "print(\"Graph visualization function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-depth visualization function ready!\n"
     ]
    }
   ],
   "source": [
    "# Multi-depth visualization function (supports depth 1-5)\n",
    "def visualize_by_depth(builder: GraphBuilder, max_nodes: int = 1000, max_depth: int = 5):\n",
    "    \"\"\"Visualize graph colored by discovery depth (supports up to depth 5).\"\"\"\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Build graph\n",
    "    for edge in builder.edges:\n",
    "        G.add_edge(edge.src_user_id, edge.dst_user_id, \n",
    "                   depth=edge.depth, interaction_type=edge.interaction_type)\n",
    "    \n",
    "    # Add node attributes\n",
    "    for node_id in G.nodes():\n",
    "        if node_id in builder.nodes:\n",
    "            node = builder.nodes[node_id]\n",
    "            G.nodes[node_id]['handle'] = node.handle\n",
    "            G.nodes[node_id]['is_root'] = node.is_root\n",
    "            G.nodes[node_id]['is_candidate'] = node.is_candidate\n",
    "            G.nodes[node_id]['discovered_via'] = node.discovered_via\n",
    "            \n",
    "            # Parse depth from discovered_via\n",
    "            via = node.discovered_via\n",
    "            if node.is_root:\n",
    "                G.nodes[node_id]['depth'] = 0\n",
    "            elif via == \"followed_by_root\" or via in [\"retweeted_root\", \"replied_to_root\"]:\n",
    "                G.nodes[node_id]['depth'] = 1\n",
    "            elif via.startswith(\"depth\") and via.endswith(\"_following\"):\n",
    "                try:\n",
    "                    G.nodes[node_id]['depth'] = int(via.replace(\"depth\", \"\").replace(\"_following\", \"\"))\n",
    "                except:\n",
    "                    G.nodes[node_id]['depth'] = 99\n",
    "            else:\n",
    "                G.nodes[node_id]['depth'] = 99\n",
    "    \n",
    "    # Sample if needed\n",
    "    if G.number_of_nodes() > max_nodes:\n",
    "        root_ids = {n.user_id for n in builder.nodes.values() if n.is_root}\n",
    "        nodes_to_keep = set(root_ids & set(G.nodes()))\n",
    "        node_degrees = dict(G.degree())\n",
    "        sorted_nodes = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "        for node_id, _ in sorted_nodes:\n",
    "            if len(nodes_to_keep) >= max_nodes:\n",
    "                break\n",
    "            nodes_to_keep.add(node_id)\n",
    "        G = G.subgraph(nodes_to_keep).copy()\n",
    "    \n",
    "    print(f\"Visualizing {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, k=2.0/len(G.nodes())**0.5, iterations=50, seed=42)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 14))\n",
    "    \n",
    "    # Color palette for depths\n",
    "    depth_colors = {\n",
    "        0: '#E74C3C',   # Red - Seeds\n",
    "        1: '#3498DB',   # Blue - Depth 1\n",
    "        2: '#9B59B6',   # Purple - Depth 2\n",
    "        3: '#27AE60',   # Green - Depth 3\n",
    "        4: '#F39C12',   # Orange - Depth 4\n",
    "        5: '#1ABC9C',   # Teal - Depth 5\n",
    "        99: '#BDC3C7',  # Gray - Other\n",
    "    }\n",
    "    \n",
    "    depth_sizes = {0: 250, 1: 100, 2: 70, 3: 50, 4: 40, 5: 35, 99: 20}\n",
    "    \n",
    "    # Categorize by depth\n",
    "    nodes_by_depth = {d: [] for d in range(max_depth + 1)}\n",
    "    nodes_by_depth[99] = []  # Other\n",
    "    \n",
    "    for n in G.nodes():\n",
    "        d = G.nodes[n].get('depth', 99)\n",
    "        if d in nodes_by_depth:\n",
    "            nodes_by_depth[d].append(n)\n",
    "        else:\n",
    "            nodes_by_depth[99].append(n)\n",
    "    \n",
    "    # Draw edges by depth\n",
    "    for d in range(1, max_depth + 1):\n",
    "        edges_at_depth = [(u, v) for u, v, data in G.edges(data=True) if data.get('depth', 1) == d]\n",
    "        if edges_at_depth:\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=edges_at_depth, ax=ax,\n",
    "                                  edge_color=depth_colors.get(d, '#CCCCCC'), \n",
    "                                  width=0.8, alpha=0.25, arrowsize=5)\n",
    "    \n",
    "    # Draw nodes by depth (from deepest to seeds so seeds are on top)\n",
    "    for d in sorted(nodes_by_depth.keys(), reverse=True):\n",
    "        if nodes_by_depth[d]:\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=nodes_by_depth[d], \n",
    "                                  node_color=depth_colors.get(d, '#BDC3C7'),\n",
    "                                  node_size=depth_sizes.get(d, 30), \n",
    "                                  alpha=0.7 if d > 0 else 0.95, ax=ax)\n",
    "    \n",
    "    # Labels for seeds\n",
    "    seed_labels = {n: G.nodes[n].get('handle', '')[:10] for n in nodes_by_depth[0]}\n",
    "    nx.draw_networkx_labels(G, pos, seed_labels, font_size=6, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Build legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = []\n",
    "    for d in range(max_depth + 1):\n",
    "        if nodes_by_depth[d]:\n",
    "            label = \"Seeds\" if d == 0 else f\"Depth {d}\"\n",
    "            legend_elements.append(Patch(facecolor=depth_colors[d], label=f'{label} ({len(nodes_by_depth[d])})'))\n",
    "    if nodes_by_depth[99]:\n",
    "        legend_elements.append(Patch(facecolor=depth_colors[99], label=f'Other ({len(nodes_by_depth[99])})'))\n",
    "    \n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "    \n",
    "    # Title showing the cascade\n",
    "    title_parts = [f\"{len(nodes_by_depth[0])} seeds\"]\n",
    "    for d in range(1, max_depth + 1):\n",
    "        if nodes_by_depth[d]:\n",
    "            title_parts.append(f\"{len(nodes_by_depth[d])} depth-{d}\")\n",
    "    \n",
    "    ax.set_title(f'Taste Graph: {\"  \".join(title_parts)}', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/processed/graph_by_depth.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Saved to data/processed/graph_by_depth.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return nodes_by_depth\n",
    "\n",
    "print(\"Multi-depth visualization function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4a - Fast LLM Screening\n",
    "\n",
    "Use `grok-4-1-fast-non-reasoning` to quickly filter candidates based on:\n",
    "- Bio\n",
    "- Pinned tweet (if available)\n",
    "\n",
    "**First LLM calls happen here!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator initialized\n",
      "  Min followers: 50\n",
      "  Max followers: 50000\n",
      "  Min tweets: 50\n",
      "  Seed handles excluded: 89\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = CandidateEvaluator(\n",
    "    x_client=x_client,\n",
    "    grok_client=grok_client,\n",
    "    criteria_path=CRITERIA_PATH,\n",
    "    min_followers=settings.get(\"min_followers_candidate\", 50),\n",
    "    max_followers=settings.get(\"max_followers_candidate\", 50000),\n",
    "    min_tweets=50,\n",
    "    seed_handles=set(roots),  # Exclude known xAI employees\n",
    "    use_fast_screen=True,\n",
    ")\n",
    "\n",
    "print(f\"Evaluator initialized\")\n",
    "print(f\"  Min followers: {evaluator.min_followers}\")\n",
    "print(f\"  Max followers: {evaluator.max_followers}\")\n",
    "print(f\"  Min tweets: {evaluator.min_tweets}\")\n",
    "print(f\"  Seed handles excluded: {len(evaluator.seed_handles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 4a: Fast LLM Screening (underrated candidates: depth 2+, <10k followers)\n",
      "============================================================\n",
      "Underrated candidate pool:\n",
      "  Depth 2+ with <10,000 followers: 1,108\n",
      "  Depth 1 with <10,000 followers: 7,377\n",
      "candidates_to_screen 8485\n",
      "\n",
      "Screening 1000 underrated candidates...\n",
      "\n",
      "Breakdown of candidates to screen:\n",
      "  depth2_following: 59\n",
      "  depth3_following: 258\n",
      "  depth4_following: 341\n",
      "  depth5_following: 342\n",
      "  Processed 50/1000 - 22 passed so far...\n",
      "  Processed 100/1000 - 34 passed so far...\n",
      "  Processed 150/1000 - 46 passed so far...\n",
      "  Processed 200/1000 - 65 passed so far...\n",
      "  Processed 250/1000 - 77 passed so far...\n",
      "  Processed 300/1000 - 93 passed so far...\n",
      "  Processed 350/1000 - 110 passed so far...\n",
      "  Processed 400/1000 - 139 passed so far...\n",
      "  Processed 450/1000 - 172 passed so far...\n",
      "  Processed 500/1000 - 202 passed so far...\n",
      "  Processed 550/1000 - 228 passed so far...\n",
      "  Processed 600/1000 - 247 passed so far...\n",
      "  Processed 650/1000 - 271 passed so far...\n",
      "  Processed 700/1000 - 294 passed so far...\n",
      "  Processed 750/1000 - 326 passed so far...\n",
      "  Processed 800/1000 - 358 passed so far...\n",
      "  Processed 850/1000 - 380 passed so far...\n",
      "  Processed 900/1000 - 403 passed so far...\n",
      "  Processed 950/1000 - 426 passed so far...\n",
      "  Processed 1000/1000 - 457 passed so far...\n",
      "\n",
      "============================================================\n",
      "FAST SCREEN COMPLETE: 457/1000 passed (45.7%)\n",
      "============================================================\n",
      "\n",
      "Pass rate by depth:\n",
      "  depth5_following: 105/342 passed (30.7%)\n",
      "  depth4_following: 177/341 passed (51.9%)\n",
      "  depth3_following: 139/258 passed (53.9%)\n",
      "  depth2_following: 36/59 passed (61.0%)\n",
      "\n",
      "Passed underrated candidates (457):\n",
      "   @austen_liao (50 followers, depth3_following) - research\n",
      "   @zzzoooeee321 (53 followers, depth3_following) - research\n",
      "   @rajivranjanmars (54 followers, depth5_following) - engineering\n",
      "   @j777ro (54 followers, depth3_following) - research\n",
      "   @ShuibaiZ69721 (60 followers, depth3_following) - research\n",
      "   @BairoliyaShivam (62 followers, depth3_following) - engineering\n",
      "   @Gooskying (64 followers, depth4_following) - research\n",
      "   @dan_sci_phil (66 followers, depth5_following) - research\n",
      "   @SeKim1112 (66 followers, depth4_following) - research\n",
      "   @infiniter3grets (68 followers, depth5_following) - engineering\n",
      "   @tangerinecoder (71 followers, depth4_following) - research\n",
      "   @allanjienlp (76 followers, depth4_following) - research\n",
      "   @nifleisch (77 followers, depth2_following) - research\n",
      "   @uccl_proj (81 followers, depth3_following) - infrastructure\n",
      "   @Ani_nlp (84 followers, depth2_following) - infrastructure\n",
      "   @Shaofeng_Yin (86 followers, depth4_following) - research\n",
      "   @tanpoporamen (87 followers, depth5_following) - engineering\n",
      "   @yash___b (90 followers, depth4_following) - research\n",
      "   @riyavsinha (90 followers, depth4_following) - research\n",
      "   @OliverasPatrick (91 followers, depth4_following) - engineering\n",
      "   @juleszqiu (92 followers, depth2_following) - research\n",
      "   @BillZheng155508 (92 followers, depth2_following) - research\n",
      "   @Yeabsira_001 (93 followers, depth5_following) - infrastructure\n",
      "   @andrewwangaw (94 followers, depth3_following) - research\n",
      "   @yingke_wang18 (95 followers, depth4_following) - engineering\n",
      "  ... and 432 more\n"
     ]
    }
   ],
   "source": [
    "# Fast screening - PRIORITIZE UNDERRATED CANDIDATES\n",
    "# 1. Depth 2+ (not direct follows - hidden gems)\n",
    "# 2. Under 10k followers (truly underrated)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 4a: Fast LLM Screening (underrated candidates: depth 2+, <10k followers)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter for underrated candidates: depth 2+ AND under 10k followers\n",
    "UNDERRATED_MAX_FOLLOWERS = 10000\n",
    "\n",
    "underrated_candidates = [\n",
    "    c for c in candidates \n",
    "    if c.get(\"discovered_via\", \"\").startswith(\"depth\") \n",
    "    and c.get(\"followers_count\", 0) < UNDERRATED_MAX_FOLLOWERS\n",
    "    and c.get(\"followers_count\", 0) >= 50  # Min threshold\n",
    "]\n",
    "\n",
    "# Also include some depth-1 candidates with low followers for comparison\n",
    "depth1_underrated = [\n",
    "    c for c in candidates\n",
    "    if c.get(\"discovered_via\") in [\"followed_by_root\", \"retweeted_root\", \"replied_to_root\"]\n",
    "    and c.get(\"followers_count\", 0) < UNDERRATED_MAX_FOLLOWERS\n",
    "    and c.get(\"followers_count\", 0) >= 50\n",
    "]\n",
    "\n",
    "print(f\"Underrated candidate pool:\")\n",
    "print(f\"  Depth 2+ with <{UNDERRATED_MAX_FOLLOWERS:,} followers: {len(underrated_candidates):,}\")\n",
    "print(f\"  Depth 1 with <{UNDERRATED_MAX_FOLLOWERS:,} followers: {len(depth1_underrated):,}\")\n",
    "\n",
    "# Prioritize depth 2+ first, then depth 1\n",
    "candidates_to_screen = underrated_candidates + depth1_underrated\n",
    "\n",
    "# Sort by depth (deeper = more underrated)\n",
    "depth_priority = {\n",
    "    \"depth5_following\": 0,\n",
    "    \"depth4_following\": 1,\n",
    "    \"depth3_following\": 2,\n",
    "    \"depth2_following\": 3,\n",
    "    \"followed_by_root\": 4,\n",
    "    \"retweeted_root\": 5,\n",
    "    \"replied_to_root\": 6,\n",
    "}\n",
    "candidates_to_screen.sort(key=lambda c: (\n",
    "    depth_priority.get(c.get(\"discovered_via\", \"\"), 99),\n",
    "    c.get(\"followers_count\", 0)  # Within same depth, lower followers first\n",
    "))\n",
    "print(\"candidates_to_screen\", len(candidates_to_screen))\n",
    "\n",
    "# How many to screen\n",
    "MAX_FAST_SCREEN = 1000\n",
    "\n",
    "print(f\"\\nScreening {min(MAX_FAST_SCREEN, len(candidates_to_screen))} underrated candidates...\")\n",
    "print(f\"\\nBreakdown of candidates to screen:\")\n",
    "for via in [\"depth2_following\", \"depth3_following\", \"depth4_following\", \"depth5_following\", \"followed_by_root\"]:\n",
    "    count = sum(1 for c in candidates_to_screen[:MAX_FAST_SCREEN] if c.get(\"discovered_via\") == via)\n",
    "    if count > 0:\n",
    "        print(f\"  {via}: {count}\")\n",
    "\n",
    "fast_screen_results = []\n",
    "depth_pass_counts = {}\n",
    "depth_fail_counts = {}\n",
    "\n",
    "for i, candidate in enumerate(candidates_to_screen[:MAX_FAST_SCREEN]):\n",
    "    handle = candidate[\"handle\"]\n",
    "    bio = candidate[\"bio\"]\n",
    "    via = candidate.get(\"discovered_via\", \"unknown\")\n",
    "    followers = candidate.get(\"followers_count\", 0)\n",
    "    \n",
    "    # Run fast screen\n",
    "    result = grok_client.fast_screen(\n",
    "        handle=handle,\n",
    "        bio=bio,\n",
    "        pinned_tweet=None,\n",
    "        location=None,\n",
    "    )\n",
    "    \n",
    "    fast_screen_results.append((candidate, result))\n",
    "    \n",
    "    # Track by depth\n",
    "    if result.pass_filter:\n",
    "        depth_pass_counts[via] = depth_pass_counts.get(via, 0) + 1\n",
    "    else:\n",
    "        depth_fail_counts[via] = depth_fail_counts.get(via, 0) + 1\n",
    "    \n",
    "    # Progress update every 50\n",
    "    if (i + 1) % 50 == 0:\n",
    "        passed = sum(1 for _, r in fast_screen_results if r.pass_filter)\n",
    "        print(f\"  Processed {i+1}/{min(MAX_FAST_SCREEN, len(candidates_to_screen))} - {passed} passed so far...\")\n",
    "\n",
    "# Summary\n",
    "passed = sum(1 for _, r in fast_screen_results if r.pass_filter)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FAST SCREEN COMPLETE: {passed}/{len(fast_screen_results)} passed ({100*passed/len(fast_screen_results):.1f}%)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nPass rate by depth:\")\n",
    "for via in [\"depth5_following\", \"depth4_following\", \"depth3_following\", \"depth2_following\", \"followed_by_root\"]:\n",
    "    p = depth_pass_counts.get(via, 0)\n",
    "    f = depth_fail_counts.get(via, 0)\n",
    "    total = p + f\n",
    "    if total > 0:\n",
    "        print(f\"  {via}: {p}/{total} passed ({100*p/total:.1f}%)\")\n",
    "\n",
    "# Show passed candidates with follower counts\n",
    "passed_candidates = [(c, r) for c, r in fast_screen_results if r.pass_filter]\n",
    "print(f\"\\nPassed underrated candidates ({len(passed_candidates)}):\")\n",
    "for c, r in sorted(passed_candidates, key=lambda x: x[0].get(\"followers_count\", 0))[:25]:\n",
    "    followers = c.get(\"followers_count\", 0)\n",
    "    print(f\"   @{c['handle']} ({followers:,} followers, {c['discovered_via']}) - {r.potential_role}\")\n",
    "if len(passed_candidates) > 25:\n",
    "    print(f\"  ... and {len(passed_candidates) - 25} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GRAPH VISUALIZATION (Post Fast-Screen)\n",
      "============================================================\n",
      "Visualizing 10000 nodes, 21039 edges\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Show multi-depth graph\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m nodes_by_depth = \u001b[43mvisualize_by_depth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Summary of LLM screening results\u001b[39;00m\n\u001b[32m     10\u001b[39m passed_handles = {c[\u001b[33m\"\u001b[39m\u001b[33mhandle\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c, r \u001b[38;5;129;01min\u001b[39;00m fast_screen_results \u001b[38;5;28;01mif\u001b[39;00m r.pass_filter}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mvisualize_by_depth\u001b[39m\u001b[34m(builder, max_nodes, max_depth)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVisualizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG.number_of_nodes()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nodes, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG.number_of_edges()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m edges\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Layout\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m pos = \u001b[43mnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspring_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m/\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m fig, ax = plt.subplots(figsize=(\u001b[32m18\u001b[39m, \u001b[32m14\u001b[39m))\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Color palette for depths\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 8:4\u001b[39m, in \u001b[36margmap_spring_layout_5\u001b[39m\u001b[34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed, store_pos_as, method, gravity)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/networkx/drawing/layout.py:634\u001b[39m, in \u001b[36mspring_layout\u001b[39m\u001b[34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed, store_pos_as, method, gravity)\u001b[39m\n\u001b[32m    632\u001b[39m         nnodes, _ = A.shape\n\u001b[32m    633\u001b[39m         k = dom_size / np.sqrt(nnodes)\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     pos = \u001b[43m_sparse_fruchterman_reingold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgravity\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    638\u001b[39m     A = nx.to_numpy_array(G, weight=weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<class 'networkx.utils.decorators.argmap'> compilation 16:4\u001b[39m, in \u001b[36margmap__sparse_fruchterman_reingold_13\u001b[39m\u001b[34m(A, k, pos, fixed, iterations, threshold, dim, seed, method, gravity)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgzip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/networkx/drawing/layout.py:764\u001b[39m, in \u001b[36m_sparse_fruchterman_reingold\u001b[39m\u001b[34m(A, k, pos, fixed, iterations, threshold, dim, seed, method, gravity)\u001b[39m\n\u001b[32m    761\u001b[39m     k = np.sqrt(\u001b[32m1.0\u001b[39m / nnodes)\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33menergy\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_energy_fruchterman_reingold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgravity\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[38;5;66;03m# make sure we have a LIst of Lists representation\u001b[39;00m\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/networkx/drawing/layout.py:871\u001b[39m, in \u001b[36m_energy_fruchterman_reingold\u001b[39m\u001b[34m(A, nnodes, k, pos, fixed, iterations, threshold, dim, gravity)\u001b[39m\n\u001b[32m    869\u001b[39m \u001b[38;5;66;03m# Optimization of the energy function by L-BFGS algorithm\u001b[39;00m\n\u001b[32m    870\u001b[39m options = {\u001b[33m\"\u001b[39m\u001b[33mmaxiter\u001b[39m\u001b[33m\"\u001b[39m: iterations, \u001b[33m\"\u001b[39m\u001b[33mgtol\u001b[39m\u001b[33m\"\u001b[39m: threshold}\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msp\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_cost_FR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mL-BFGS-B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.x.reshape((nnodes, dim))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/scipy/optimize/_minimize.py:784\u001b[39m, in \u001b[36mminimize\u001b[39m\u001b[34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[39m\n\u001b[32m    781\u001b[39m     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[32m    782\u001b[39m                              **options)\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33ml-bfgs-b\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     res = \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m meth == \u001b[33m'\u001b[39m\u001b[33mtnc\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    787\u001b[39m     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[32m    788\u001b[39m                         **options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/scipy/optimize/_lbfgsb_py.py:469\u001b[39m, in \u001b[36m_minimize_lbfgsb\u001b[39m\u001b[34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, workers, **unknown_options)\u001b[39m\n\u001b[32m    461\u001b[39m _lbfgsb.setulb(m, x, low_bnd, upper_bnd, nbd, f, g, factr, pgtol, wa,\n\u001b[32m    462\u001b[39m                iwa, task, lsave, isave, dsave, maxls, ln_task)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m3\u001b[39m:\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     f, g = \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m task[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[32m    472\u001b[39m     n_iterations += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:403\u001b[39m, in \u001b[36mScalarFunction.fun_and_grad\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.array_equal(x, \u001b[38;5;28mself\u001b[39m.x):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_x(x)\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28mself\u001b[39m._update_grad()\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f, \u001b[38;5;28mself\u001b[39m.g\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/scipy/optimize/_differentiable_functions.py:353\u001b[39m, in \u001b[36mScalarFunction._update_fun\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    352\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.f_updated:\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m         fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28mself\u001b[39m._nfev += \u001b[32m1\u001b[39m\n\u001b[32m    355\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fx < \u001b[38;5;28mself\u001b[39m._lowest_f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/scipy/_lib/_util.py:590\u001b[39m, in \u001b[36m_ScalarFunctionWrapper.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;66;03m# The user of this class might want `x` to remain unchanged.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m     fx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28mself\u001b[39m.nfev += \u001b[32m1\u001b[39m\n\u001b[32m    593\u001b[39m     \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/scipy/optimize/_optimize.py:80\u001b[39m, in \u001b[36mMemoizeJac.__call__\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, *args):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/scipy/optimize/_optimize.py:74\u001b[39m, in \u001b[36mMemoizeJac._compute_if_needed\u001b[39m\u001b[34m(self, x, *args)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all(x == \u001b[38;5;28mself\u001b[39m.x) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mself\u001b[39m.x = np.asarray(x).copy()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     fg = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m.jac = fg[\u001b[32m1\u001b[39m]\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m._value = fg[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/networkx/drawing/layout.py:848\u001b[39m, in \u001b[36m_energy_fruchterman_reingold.<locals>._cost_FR\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    846\u001b[39m delta = pos[l:r, np.newaxis, :] - pos[np.newaxis, :, :]\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# distance between points with a minimum distance of 1e-5\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m distance2 = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    849\u001b[39m distance2 = np.maximum(distance2, \u001b[32m1e-10\u001b[39m)\n\u001b[32m    850\u001b[39m distance = np.sqrt(distance2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/xai-hack/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:2333\u001b[39m, in \u001b[36m_sum_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[33m'\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m'\u001b[39m, a_min, a_max, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2334\u001b[39m                     initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   2338\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[32m   2339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   2340\u001b[39m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # Visualize graph by depth (after fast LLM screening)\n",
    "# print(\"=\" * 60)\n",
    "# print(\"GRAPH VISUALIZATION (Post Fast-Screen)\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # Show multi-depth graph\n",
    "# nodes_by_depth = visualize_by_depth(builder, max_nodes=10000, max_depth=5)\n",
    "\n",
    "# # Summary of LLM screening results\n",
    "# passed_handles = {c[\"handle\"] for c, r in fast_screen_results if r.pass_filter}\n",
    "# failed_handles = {c[\"handle\"] for c, r in fast_screen_results if not r.pass_filter}\n",
    "\n",
    "# print(f\"\\nFast LLM Screening Summary:\")\n",
    "# print(f\"  Total screened: {len(fast_screen_results)}\")\n",
    "# print(f\"  Passed (engineering candidates): {len(passed_handles)}\")\n",
    "# print(f\"  Failed (non-technical/filtered): {len(failed_handles)}\")\n",
    "\n",
    "# # Show depth breakdown of passed candidates\n",
    "# print(f\"\\nPassed candidates by depth:\")\n",
    "# passed_by_depth = {}\n",
    "# for c, r in fast_screen_results:\n",
    "#     if r.pass_filter:\n",
    "#         via = c.get(\"discovered_via\", \"unknown\")\n",
    "#         passed_by_depth[via] = passed_by_depth.get(via, 0) + 1\n",
    "# for via, count in sorted(passed_by_depth.items()):\n",
    "#     print(f\"  {via}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4b - Full Grok Evaluation (SKIP)\n",
    "\n",
    "**SKIP THIS SECTION** - We use Deep Evaluation with xAI Search Tools instead (Phase 5).\n",
    "\n",
    "The cells below (24-26) are kept for reference but should NOT be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP - Using Deep Evaluation with xAI Search Tools instead\n",
    "# # Full evaluation on candidates that passed fast screen\n",
    "# print(\"=\" * 60)\n",
    "# print(\"PHASE 4b: Full Grok Evaluation (on fast-screen passed candidates)\")\n",
    "# print(\"=\" * 60)\n",
    "# \n",
    "# # Get candidates that passed fast screening\n",
    "# passed_candidate_handles = {c[\"handle\"] for c, r in fast_screen_results if r.pass_filter}\n",
    "# candidates_to_eval = [c for c in candidates if c[\"handle\"] in passed_candidate_handles]\n",
    "# \n",
    "# print(f\"Candidates that passed fast screen: {len(candidates_to_eval)}\")\n",
    "# \n",
    "# # How many to fully evaluate\n",
    "# MAX_EVAL = 500  # Full eval is more expensive, so limit\n",
    "# \n",
    "# results = evaluator.evaluate_batch(candidates_to_eval, max_candidates=MAX_EVAL)\n",
    "# \n",
    "# # Update nodes with evaluations\n",
    "# evaluated_count = 0\n",
    "# relevant_count = 0\n",
    "# \n",
    "# for candidate, pre_filter, evaluation in results:\n",
    "#     user_id = candidate.get(\"user_id\")\n",
    "#     if user_id in builder.nodes:\n",
    "#         node = builder.nodes[user_id]\n",
    "#         node.grok_evaluated = True\n",
    "#         evaluated_count += 1\n",
    "#         \n",
    "#         if evaluation:\n",
    "#             node.grok_relevant = evaluation.relevant\n",
    "#             node.grok_score = evaluation.score\n",
    "#             node.grok_reasoning = evaluation.reasoning\n",
    "#             node.grok_skills = \",\".join(evaluation.detected_skills)\n",
    "#             node.grok_role = evaluation.recommended_role\n",
    "#             node.grok_exceptional_work = evaluation.exceptional_work\n",
    "#             node.grok_red_flags = \",\".join(evaluation.red_flags)\n",
    "#             \n",
    "#             if evaluation.relevant:\n",
    "#                 relevant_count += 1\n",
    "#                 via = candidate.get(\"discovered_via\", \"unknown\")\n",
    "#                 print(f\"   @{node.handle} ({via}) - score: {evaluation.score:.2f} - {evaluation.recommended_role}\")\n",
    "# \n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(f\"Evaluation complete: {evaluated_count} evaluated, {relevant_count} marked relevant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP - Part of old Phase 4b (Full Grok Evaluation)\n",
    "# # Debug: Check evaluation results\n",
    "# print(\"=\" * 60)\n",
    "# print(\"EVALUATION RESULTS BREAKDOWN\")\n",
    "# print(\"=\" * 60)\n",
    "# \n",
    "# passed_prefilter = 0\n",
    "# failed_prefilter = 0\n",
    "# got_full_eval = 0\n",
    "# marked_relevant = 0\n",
    "# \n",
    "# prefilter_reasons = {}\n",
    "# \n",
    "# for candidate, pre_filter, evaluation in results:\n",
    "#     if pre_filter.passed:\n",
    "#         passed_prefilter += 1\n",
    "#         if evaluation:\n",
    "#             got_full_eval += 1\n",
    "#             if evaluation.relevant:\n",
    "#                 marked_relevant += 1\n",
    "#     else:\n",
    "#         failed_prefilter += 1\n",
    "#         reason = pre_filter.reason.split(':')[0] if ':' in pre_filter.reason else pre_filter.reason\n",
    "#         prefilter_reasons[reason] = prefilter_reasons.get(reason, 0) + 1\n",
    "# \n",
    "# print(f\"Pre-filter results:\")\n",
    "# print(f\"  Passed: {passed_prefilter}\")\n",
    "# print(f\"  Failed: {failed_prefilter}\")\n",
    "# print(f\"\\nPre-filter failure reasons:\")\n",
    "# for reason, count in sorted(prefilter_reasons.items(), key=lambda x: -x[1])[:10]:\n",
    "#     print(f\"  {reason}: {count}\")\n",
    "# \n",
    "# print(f\"\\nFull evaluation results:\")\n",
    "# print(f\"  Got full eval: {got_full_eval}\")\n",
    "# print(f\"  Marked relevant: {marked_relevant}\")\n",
    "# \n",
    "# # Show some examples of candidates that passed pre-filter but weren't marked relevant\n",
    "# print(f\"\\nExamples of passed pre-filter but not relevant:\")\n",
    "# shown = 0\n",
    "# for candidate, pre_filter, evaluation in results:\n",
    "#     if pre_filter.passed and evaluation and not evaluation.relevant and shown < 5:\n",
    "#         print(f\"  @{candidate['handle']}: {evaluation.reasoning[:80]}...\")\n",
    "#         shown += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP - Part of old Phase 4b (Full Grok Evaluation)\n",
    "# # Print evaluation summary\n",
    "# print_evaluation_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5 - Rank & Export\n",
    "\n",
    "Compute underratedness scores and export results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 5: Computing Rankings\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "COMPUTING RANKINGS\n",
      "============================================================\n",
      "  Root nodes (seeds): 89\n",
      "  Method: Personalized PageRank\n",
      "\n",
      "  Top 20 candidates by UNDERRATEDNESS score:\n",
      "  (High PageRank + Low Followers = Hidden Gem)\n",
      "  ----------------------------------------------------------------------\n",
      "   1. @theVincentStark      | followers:   2,206 | ppr: 0.002927 | underrated: 0.000380\n",
      "   2. @512x512              | followers:  47,933 | ppr: 0.003198 | underrated: 0.000297\n",
      "   3. @LiLiDuc22            | followers:   3,864 | ppr: 0.001268 | underrated: 0.000154\n",
      "   4. @JiachengHong         | followers:     531 | ppr: 0.000915 | underrated: 0.000146\n",
      "   5. @MichelleShieh        | followers:   2,916 | ppr: 0.000949 | underrated: 0.000119\n",
      "   6. @ericzelikman         | followers:  22,370 | ppr: 0.000981 | underrated: 0.000098\n",
      "   7. @arnogau              | followers:   9,326 | ppr: 0.000860 | underrated: 0.000094\n",
      "   8. @dboedijono           | followers:   1,178 | ppr: 0.000654 | underrated: 0.000092\n",
      "   9. @xu63006              | followers:     155 | ppr: 0.000434 | underrated: 0.000086\n",
      "  10. @jimmybajimmyba       | followers:  25,452 | ppr: 0.000853 | underrated: 0.000084\n",
      "  11. @HeinrichKuttler      | followers:  19,196 | ppr: 0.000741 | underrated: 0.000075\n",
      "  12. @Yuhu_ai_             | followers:  44,623 | ppr: 0.000779 | underrated: 0.000073\n",
      "  13. @bicycle4mind         | followers:     965 | ppr: 0.000489 | underrated: 0.000071\n",
      "  14. @belce_dogru          | followers:  11,174 | ppr: 0.000660 | underrated: 0.000071\n",
      "  15. @suyanus              | followers:   2,915 | ppr: 0.000549 | underrated: 0.000069\n",
      "  16. @veggie_eric          | followers:  34,353 | ppr: 0.000707 | underrated: 0.000068\n",
      "  17. @conglei_shi          | followers:     138 | ppr: 0.000318 | underrated: 0.000064\n",
      "  18. @zen0wu               | followers:     146 | ppr: 0.000318 | underrated: 0.000064\n",
      "  19. @showbufire           | followers:     150 | ppr: 0.000318 | underrated: 0.000063\n",
      "  20. @MohitReddy13         | followers:   3,759 | ppr: 0.000521 | underrated: 0.000063\n",
      "\n",
      "  Top 20 candidates by raw PAGERANK score:\n",
      "  ----------------------------------------------------------------------\n",
      "   1. @512x512              | followers:  47,933 | ppr: 0.003198\n",
      "   2. @theVincentStark      | followers:   2,206 | ppr: 0.002927\n",
      "   3. @LiLiDuc22            | followers:   3,864 | ppr: 0.001268\n",
      "   4. @ericzelikman         | followers:  22,370 | ppr: 0.000981\n",
      "   5. @MichelleShieh        | followers:   2,916 | ppr: 0.000949\n",
      "   6. @JiachengHong         | followers:     531 | ppr: 0.000915\n",
      "   7. @arnogau              | followers:   9,326 | ppr: 0.000860\n",
      "   8. @jimmybajimmyba       | followers:  25,452 | ppr: 0.000853\n",
      "   9. @Yuhu_ai_             | followers:  44,623 | ppr: 0.000779\n",
      "  10. @HeinrichKuttler      | followers:  19,196 | ppr: 0.000741\n",
      "  11. @veggie_eric          | followers:  34,353 | ppr: 0.000707\n",
      "  12. @belce_dogru          | followers:  11,174 | ppr: 0.000660\n",
      "  13. @dboedijono           | followers:   1,178 | ppr: 0.000654\n",
      "  14. @skcd42               | followers:   9,428 | ppr: 0.000559\n",
      "  15. @arsh99_singh         | followers:   9,757 | ppr: 0.000557\n",
      "  16. @suyanus              | followers:   2,915 | ppr: 0.000549\n",
      "  17. @VastoLorde95         | followers:   8,647 | ppr: 0.000533\n",
      "  18. @EthanHe_42           | followers:  26,650 | ppr: 0.000525\n",
      "  19. @MohitReddy13         | followers:   3,759 | ppr: 0.000521\n",
      "  20. @ParsaTajik           | followers:   6,034 | ppr: 0.000511\n",
      "  ----------------------------------------------------------------------\n",
      "\n",
      "  Total candidates: 12050\n"
     ]
    }
   ],
   "source": [
    "# Compute rankings\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 5: Computing Rankings\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "nodes = compute_rankings(builder.nodes, builder.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 6: Exporting Results\n",
      "============================================================\n",
      "  Exported 17323 ranked nodes to data/processed/nodes.csv\n",
      "  Exported 17323 nodes to data/processed/nodes.csv\n",
      "  Exported 28841 edges to data/processed/edges.csv\n",
      "\n",
      "Exported to:\n",
      "  data/processed/nodes.csv\n",
      "  data/processed/edges.csv\n"
     ]
    }
   ],
   "source": [
    "# Export results\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 6: Exporting Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nodes_path = output_dir / \"nodes.csv\"\n",
    "edges_path = output_dir / \"edges.csv\"\n",
    "\n",
    "export_ranked_nodes(nodes, str(nodes_path))\n",
    "builder.export_to_csv(str(nodes_path), str(edges_path))\n",
    "\n",
    "print(f\"\\nExported to:\")\n",
    "print(f\"  {nodes_path}\")\n",
    "print(f\"  {edges_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORTING GRAPH FOR PAGERANK ANALYSIS\n",
      "============================================================\n",
      "Graph built: 17,322 nodes, 28,361 edges\n",
      "   Edge list CSV: data/graph_export/edges.csv\n",
      "   Node list CSV: data/graph_export/nodes.csv\n",
      "   GraphML: data/graph_export/graph.graphml\n",
      "   GML: data/graph_export/graph.gml\n",
      "   Pickle: data/graph_export/graph.pickle\n",
      "   Adjacency list: data/graph_export/graph.adjlist\n",
      "   JSON: data/graph_export/graph.json\n",
      "\n",
      "============================================================\n",
      "EXPORT COMPLETE\n",
      "============================================================\n",
      "All files saved to: data/graph_export/\n",
      "\n",
      "Files:\n",
      "  edges.csv      - Edge list (source, target, weight) for PageRank\n",
      "  nodes.csv      - Node attributes (handle, followers, is_root, etc.)\n",
      "  graph.graphml  - Full graph for Gephi/NetworkX\n",
      "  graph.gml      - Full graph for igraph\n",
      "  graph.pickle   - Python pickle (fastest to load)\n",
      "  graph.adjlist  - Compact adjacency list\n",
      "  graph.json     - JSON for web visualization\n",
      "\n",
      "Quick PageRank example:\n",
      "  import networkx as nx\n",
      "  import pickle\n",
      "\n",
      "  with open('data/graph_export/graph.pickle', 'rb') as f:\n",
      "      G = pickle.load(f)\n",
      "\n",
      "  # Run PageRank with seeds as personalization\n",
      "  seeds = [n for n, d in G.nodes(data=True) if d.get('is_root')]\n",
      "  personalization = {n: 1.0 for n in seeds}\n",
      "  pr = nx.pagerank(G, personalization=personalization, weight='weight')\n",
      "\n",
      "  # Get top candidates by PageRank\n",
      "  candidates = [(n, pr[n]) for n, d in G.nodes(data=True) if d.get('is_candidate')]\n",
      "  top_candidates = sorted(candidates, key=lambda x: x[1], reverse=True)[:20]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Export graph in multiple formats for PageRank analysis\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORTING GRAPH FOR PAGERANK ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build NetworkX graph with all attributes\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges with weights\n",
    "for edge in builder.edges:\n",
    "    G.add_edge(\n",
    "        edge.src_user_id,\n",
    "        edge.dst_user_id,\n",
    "        weight=edge.weight,\n",
    "        interaction_type=edge.interaction_type,\n",
    "        depth=edge.depth,\n",
    "        tweet_id=edge.tweet_id,\n",
    "    )\n",
    "\n",
    "# Add node attributes\n",
    "for node_id, node in builder.nodes.items():\n",
    "    if node_id in G:\n",
    "        G.nodes[node_id].update({\n",
    "            'handle': node.handle,\n",
    "            'name': node.name,\n",
    "            'bio': node.bio[:200] if node.bio else '',  # Truncate long bios\n",
    "            'followers_count': node.followers_count,\n",
    "            'following_count': node.following_count,\n",
    "            'tweet_count': node.tweet_count,\n",
    "            'is_root': node.is_root,\n",
    "            'is_candidate': node.is_candidate,\n",
    "            'discovered_via': node.discovered_via,\n",
    "        })\n",
    "\n",
    "print(f\"Graph built: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "\n",
    "# Create output directory\n",
    "export_dir = Path(\"data/graph_export\")\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Edge list CSV (simplest format for PageRank)\n",
    "edge_list_path = export_dir / \"edges.csv\"\n",
    "with open(edge_list_path, 'w') as f:\n",
    "    f.write(\"source,target,weight,interaction_type,depth\\n\")\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        f.write(f\"{u},{v},{data.get('weight', 1.0)},{data.get('interaction_type', '')},{data.get('depth', 1)}\\n\")\n",
    "print(f\"   Edge list CSV: {edge_list_path}\")\n",
    "\n",
    "# 2. Node list CSV (with attributes)\n",
    "node_list_path = export_dir / \"nodes.csv\"\n",
    "with open(node_list_path, 'w') as f:\n",
    "    f.write(\"user_id,handle,name,followers_count,following_count,tweet_count,is_root,is_candidate,discovered_via\\n\")\n",
    "    for node_id, attrs in G.nodes(data=True):\n",
    "        handle = attrs.get('handle', '').replace(',', ' ')\n",
    "        name = attrs.get('name', '').replace(',', ' ').replace('\"', '')\n",
    "        f.write(f\"{node_id},{handle},{name},{attrs.get('followers_count', 0)},{attrs.get('following_count', 0)},{attrs.get('tweet_count', 0)},{attrs.get('is_root', False)},{attrs.get('is_candidate', False)},{attrs.get('discovered_via', '')}\\n\")\n",
    "print(f\"   Node list CSV: {node_list_path}\")\n",
    "\n",
    "# 3. GraphML (for Gephi, NetworkX - includes all attributes)\n",
    "graphml_path = export_dir / \"graph.graphml\"\n",
    "nx.write_graphml(G, graphml_path)\n",
    "print(f\"   GraphML: {graphml_path}\")\n",
    "\n",
    "# 4. GML (for igraph, NetworkX)\n",
    "# Note: GML doesn't support all attribute types, so we simplify\n",
    "G_gml = G.copy()\n",
    "for node_id in G_gml.nodes():\n",
    "    # Convert booleans to int for GML compatibility\n",
    "    G_gml.nodes[node_id]['is_root'] = int(G_gml.nodes[node_id].get('is_root', False))\n",
    "    G_gml.nodes[node_id]['is_candidate'] = int(G_gml.nodes[node_id].get('is_candidate', False))\n",
    "gml_path = export_dir / \"graph.gml\"\n",
    "nx.write_gml(G_gml, gml_path)\n",
    "print(f\"   GML: {gml_path}\")\n",
    "\n",
    "# 5. Pickle (fastest to load in Python)\n",
    "pickle_path = export_dir / \"graph.pickle\"\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "print(f\"   Pickle: {pickle_path}\")\n",
    "\n",
    "# 6. Adjacency list (compact text format)\n",
    "adjlist_path = export_dir / \"graph.adjlist\"\n",
    "nx.write_adjlist(G, adjlist_path)\n",
    "print(f\"   Adjacency list: {adjlist_path}\")\n",
    "\n",
    "# 7. JSON format (for web visualization / JavaScript)\n",
    "json_data = {\n",
    "    \"nodes\": [\n",
    "        {\n",
    "            \"id\": node_id,\n",
    "            **{k: v for k, v in attrs.items() if k != 'bio'}  # Exclude long bios\n",
    "        }\n",
    "        for node_id, attrs in G.nodes(data=True)\n",
    "    ],\n",
    "    \"edges\": [\n",
    "        {\"source\": u, \"target\": v, **data}\n",
    "        for u, v, data in G.edges(data=True)\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"total_nodes\": G.number_of_nodes(),\n",
    "        \"total_edges\": G.number_of_edges(),\n",
    "        \"roots\": sum(1 for _, d in G.nodes(data=True) if d.get('is_root')),\n",
    "        \"candidates\": sum(1 for _, d in G.nodes(data=True) if d.get('is_candidate')),\n",
    "    }\n",
    "}\n",
    "json_path = export_dir / \"graph.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(json_data, f)\n",
    "print(f\"   JSON: {json_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"All files saved to: {export_dir}/\")\n",
    "print(f\"\"\"\n",
    "Files:\n",
    "  edges.csv      - Edge list (source, target, weight) for PageRank\n",
    "  nodes.csv      - Node attributes (handle, followers, is_root, etc.)\n",
    "  graph.graphml  - Full graph for Gephi/NetworkX\n",
    "  graph.gml      - Full graph for igraph\n",
    "  graph.pickle   - Python pickle (fastest to load)\n",
    "  graph.adjlist  - Compact adjacency list\n",
    "  graph.json     - JSON for web visualization\n",
    "\n",
    "Quick PageRank example:\n",
    "  import networkx as nx\n",
    "  import pickle\n",
    "  \n",
    "  with open('data/graph_export/graph.pickle', 'rb') as f:\n",
    "      G = pickle.load(f)\n",
    "  \n",
    "  # Run PageRank with seeds as personalization\n",
    "  seeds = [n for n, d in G.nodes(data=True) if d.get('is_root')]\n",
    "  personalization = {{n: 1.0 for n in seeds}}\n",
    "  pr = nx.pagerank(G, personalization=personalization, weight='weight')\n",
    "  \n",
    "  # Get top candidates by PageRank\n",
    "  candidates = [(n, pr[n]) for n, d in G.nodes(data=True) if d.get('is_candidate')]\n",
    "  top_candidates = sorted(candidates, key=lambda x: x[1], reverse=True)[:20]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Graph for PageRank Analysis\n",
    "\n",
    "Export the network graph in multiple formats for your teammate to run PageRank:\n",
    "- **Edge list CSV** - Simple (source, target, weight) format\n",
    "- **GraphML** - Rich XML format with node attributes (for Gephi, NetworkX)\n",
    "- **GML** - Graph Modelling Language (for igraph, NetworkX)\n",
    "- **Adjacency list** - Compact text format\n",
    "- **Pickle** - Python NetworkX graph object (fastest to load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE SUMMARY (Pre-Deep Evaluation)\n",
      "============================================================\n",
      "\n",
      "Graph Discovery:\n",
      "  Total accounts discovered: 17,323\n",
      "  Total interactions mapped: 28,841\n",
      "  Candidate accounts: 12,050\n",
      "\n",
      "Fast LLM Screening:\n",
      "  Screened: 1000\n",
      "  Passed: 457\n",
      "  Failed: 543\n",
      "  Pass rate: 45.7%\n",
      "\n",
      "Next: Run PageRank + Deep Evaluation with xAI Search Tools\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final summary (after fast screen, before deep eval)\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE SUMMARY (Pre-Deep Evaluation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count from fast screen results\n",
    "passed_count = sum(1 for _, r in fast_screen_results if r.pass_filter)\n",
    "failed_count = sum(1 for _, r in fast_screen_results if not r.pass_filter)\n",
    "\n",
    "print(f\"\"\"\n",
    "Graph Discovery:\n",
    "  Total accounts discovered: {len(builder.nodes):,}\n",
    "  Total interactions mapped: {len(builder.edges):,}\n",
    "  Candidate accounts: {sum(1 for n in builder.nodes.values() if n.is_candidate):,}\n",
    "\n",
    "Fast LLM Screening:\n",
    "  Screened: {len(fast_screen_results)}\n",
    "  Passed: {passed_count}\n",
    "  Failed: {failed_count}\n",
    "  Pass rate: {100*passed_count/len(fast_screen_results):.1f}%\n",
    "\n",
    "Next: Run PageRank + Deep Evaluation with xAI Search Tools\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Top Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP CANDIDATES FROM FAST SCREEN\n",
      "============================================================\n",
      "\n",
      "Total passed: 457\n",
      "\n",
      "Top 20 most underrated (lowest followers):\n",
      "------------------------------------------------------------\n",
      " 1. @austen_liao          |    50 followers | depth3_following | research\n",
      " 2. @zzzoooeee321         |    53 followers | depth3_following | research\n",
      " 3. @rajivranjanmars      |    54 followers | depth5_following | engineering\n",
      " 4. @j777ro               |    54 followers | depth3_following | research\n",
      " 5. @ShuibaiZ69721        |    60 followers | depth3_following | research\n",
      " 6. @BairoliyaShivam      |    62 followers | depth3_following | engineering\n",
      " 7. @Gooskying            |    64 followers | depth4_following | research\n",
      " 8. @dan_sci_phil         |    66 followers | depth5_following | research\n",
      " 9. @SeKim1112            |    66 followers | depth4_following | research\n",
      "10. @infiniter3grets      |    68 followers | depth5_following | engineering\n",
      "11. @tangerinecoder       |    71 followers | depth4_following | research\n",
      "12. @allanjienlp          |    76 followers | depth4_following | research\n",
      "13. @nifleisch            |    77 followers | depth2_following | research\n",
      "14. @uccl_proj            |    81 followers | depth3_following | infrastructure\n",
      "15. @Ani_nlp              |    84 followers | depth2_following | infrastructure\n",
      "16. @Shaofeng_Yin         |    86 followers | depth4_following | research\n",
      "17. @tanpoporamen         |    87 followers | depth5_following | engineering\n",
      "18. @yash___b             |    90 followers | depth4_following | research\n",
      "19. @riyavsinha           |    90 followers | depth4_following | research\n",
      "20. @OliverasPatrick      |    91 followers | depth4_following | engineering\n",
      "------------------------------------------------------------\n",
      "\n",
      "Ready for PageRank + Deep Evaluation\n"
     ]
    }
   ],
   "source": [
    "# Show top candidates from fast screen (before deep eval)\n",
    "print(\"TOP CANDIDATES FROM FAST SCREEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "passed_candidates = [(c, r) for c, r in fast_screen_results if r.pass_filter]\n",
    "\n",
    "# Sort by followers (lowest first = most underrated)\n",
    "passed_candidates.sort(key=lambda x: x[0].get('followers_count', 0))\n",
    "\n",
    "print(f\"\\nTotal passed: {len(passed_candidates)}\")\n",
    "print(f\"\\nTop 20 most underrated (lowest followers):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (c, r) in enumerate(passed_candidates[:20], 1):\n",
    "    handle = c.get('handle', 'unknown')\n",
    "    followers = c.get('followers_count', 0)\n",
    "    via = c.get('discovered_via', 'unknown')\n",
    "    role = r.potential_role\n",
    "    print(f\"{i:2}. @{handle:<20} | {followers:>5,} followers | {via} | {role}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"\\nReady for PageRank + Deep Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Deep Evaluation with xAI Search Tools\n",
    "\n",
    "Use `grok-4-1-fast` with `web_search` + `x_search` to deeply evaluate top candidates:\n",
    "- Search X for their posts and discussions\n",
    "- Search GitHub for their repos\n",
    "- Search LinkedIn for their background\n",
    "- Score on 5-criterion rubric (0-100 scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 6: Deep Evaluation with xAI Search Tools\n",
      "============================================================\n",
      "Loaded graph: 17,322 nodes, 28,361 edges\n",
      "Computing PageRank with 88 seeds as personalization...\n",
      "PageRank computed for 17322 nodes\n",
      "\n",
      "Top 20 underrated candidates for deep eval:\n",
      "  1. @juleszqiu (92 followers, PPR: 0.000067)\n",
      "  2. @WesleyYue (534 followers, PPR: 0.000081)\n",
      "  3. @Ani_nlp (84 followers, PPR: 0.000046)\n",
      "  4. @BahlAnuraag (531 followers, PPR: 0.000057)\n",
      "  5. @tianyue_01 (100 followers, PPR: 0.000041)\n",
      "  6. @guangyi_l (380 followers, PPR: 0.000046)\n",
      "  7. @alexzhuang_ (100 followers, PPR: 0.000033)\n",
      "  8. @LiaoZeyi (312 followers, PPR: 0.000041)\n",
      "  9. @MatricesAI (422 followers, PPR: 0.000041)\n",
      "  10. @einsums (167 followers, PPR: 0.000034)\n",
      "  11. @shmkane (504 followers, PPR: 0.000041)\n",
      "  12. @nifleisch (77 followers, PPR: 0.000027)\n",
      "  13. @BillZheng155508 (92 followers, PPR: 0.000027)\n",
      "  14. @the_philbert (330 followers, PPR: 0.000034)\n",
      "  15. @mihiranand (492 followers, PPR: 0.000033)\n",
      "  16. @kauterry (165 followers, PPR: 0.000027)\n",
      "  17. @jaewon_chung_cs (243 followers, PPR: 0.000027)\n",
      "  18. @j_m_sommer (262 followers, PPR: 0.000027)\n",
      "  19. @JunlinWang3 (265 followers, PPR: 0.000027)\n",
      "  20. @lightetal (266 followers, PPR: 0.000027)\n",
      "\n",
      "============================================================\n",
      "Running deep evaluation on top 50 candidates...\n",
      "This uses web_search + x_search for each candidate\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Phase 6: Deep Evaluation using xAI Search Tools\n",
    "# Uses grok-4-1-fast with web_search + x_search to deeply analyze candidates\n",
    "\n",
    "from src.deep_evaluator import DeepEvaluator, print_evaluation\n",
    "from src.ranking import compute_pagerank, get_top_candidates\n",
    "import pickle\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 6: Deep Evaluation with xAI Search Tools\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load graph for PageRank\n",
    "with open('data/graph_export/graph.pickle', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "\n",
    "# Compute PageRank with seed personalization\n",
    "seed_ids = [n for n, d in G.nodes(data=True) if d.get('is_root')]\n",
    "print(f\"Computing PageRank with {len(seed_ids)} seeds as personalization...\")\n",
    "\n",
    "pagerank_scores = compute_pagerank(G, seed_ids)\n",
    "print(f\"PageRank computed for {len(pagerank_scores)} nodes\")\n",
    "\n",
    "# Get top candidates by underratedness (PageRank / log(followers))\n",
    "# Focus on candidates that passed fast screening\n",
    "passed_handles = {c[\"handle\"] for c, r in fast_screen_results if r.pass_filter}\n",
    "\n",
    "candidates_for_deep_eval = []\n",
    "for node_id, ppr_score in pagerank_scores.items():\n",
    "    if node_id not in G.nodes:\n",
    "        continue\n",
    "    attrs = G.nodes[node_id]\n",
    "    if not attrs.get('is_candidate'):\n",
    "        continue\n",
    "    \n",
    "    handle = attrs.get('handle', '')\n",
    "    if handle not in passed_handles:\n",
    "        continue\n",
    "    \n",
    "    followers = attrs.get('followers_count', 1)\n",
    "    # Underratedness = PPR / log(1 + followers)\n",
    "    import math\n",
    "    underratedness = ppr_score / math.log(1 + followers)\n",
    "    \n",
    "    candidates_for_deep_eval.append({\n",
    "        'user_id': node_id,\n",
    "        'handle': handle,\n",
    "        'bio': builder.nodes[node_id].bio if node_id in builder.nodes else '',\n",
    "        'followers_count': followers,\n",
    "        'pagerank_score': ppr_score,\n",
    "        'underratedness': underratedness,\n",
    "        'discovered_via': attrs.get('discovered_via', ''),\n",
    "    })\n",
    "\n",
    "# Sort by underratedness (hidden gems first)\n",
    "candidates_for_deep_eval.sort(key=lambda x: x['underratedness'], reverse=True)\n",
    "\n",
    "print(f\"\\nTop 20 underrated candidates for deep eval:\")\n",
    "for i, c in enumerate(candidates_for_deep_eval[:20], 1):\n",
    "    print(f\"  {i}. @{c['handle']} ({c['followers_count']:,} followers, PPR: {c['pagerank_score']:.6f})\")\n",
    "\n",
    "# How many to deep evaluate\n",
    "MAX_DEEP_EVAL = 50  # Deep eval is expensive (uses search tools)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Running deep evaluation on top {MAX_DEEP_EVAL} candidates...\")\n",
    "print(\"This uses web_search + x_search for each candidate\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepEvaluator initialized successfully\n",
      "  Model: grok-4-1-fast\n",
      "  Cache dir: data/enriched\n",
      "\n",
      "[deep_eval] Evaluating 50 candidates\n",
      "============================================================\n",
      "\n",
      "[1/50] @juleszqiu\n",
      "  [eval] @juleszqiu - searching X, GitHub, LinkedIn...\n",
      "    -> x_keyword_search: {\"query\":\"from:juleszqiu\",\"limit\":30,\"mode\":\"Lates...\n",
      "    -> x_keyword_search: {\"query\":\"from:juleszqiu (AI OR ML OR \\\"machine le...\n",
      "    -> x_semantic_search: {\"query\":\"technical discussions on AI, math, engin...\n",
      "    -> x_user_search: {\"query\":\"juleszqiu\"}...\n",
      "    -> web_search: {\"query\":\"\\\"juleszqiu\\\" site:github.com\",\"num_resu...\n",
      "    -> web_search: {\"query\":\"\\\"jules qiu\\\" github\",\"num_results\":10}...\n",
      "    -> web_search: {\"query\":\"\\\"jules qiu\\\" site:linkedin.com/in\",\"num...\n",
      "    -> web_search: {\"query\":\"\\\"juleszqiu\\\" site:linkedin.com\",\"num_re...\n",
      "    -> browse_page: {\"url\": \"https://www.linkedin.com/in/jules-qiu\"}...\n",
      "    -> web_search: {\"query\":\"\\\"jules qiu\\\" radical ventures github OR...\n",
      "    -> web_search: {\"query\":\"\\\"jules qiu\\\" harvard math thesis OR pap...\n",
      "    -> web_search: {\"query\":\"site:radical.vc \\\"jules qiu\\\"\"}...\n",
      "    -> x_keyword_search: {\"query\":\"from:juleszqiu\",\"limit\":50,\"mode\":\"Top\"}...\n",
      "    -> web_search: {\"query\":\"\\\"jules qiu\\\" site:arxiv.org OR site:sch...\n",
      "[error] Deep evaluation failed: Object of type RepeatedScalarContainer is not JSON serializable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/6j/q7vlb7k917jcgd428x526prc0000gn/T/ipykernel_64163/4149176246.py\", line 9, in <module>\n",
      "    deep_results = deep_evaluator.evaluate_batch(\n",
      "        candidates=candidates_for_deep_eval,\n",
      "        max_candidates=MAX_DEEP_EVAL,\n",
      "        verbose=True,\n",
      "    )\n",
      "  File \"/Users/hungtran/code/xai-hack/src/deep_evaluator.py\", line 388, in evaluate_batch\n",
      "    if verbose:\n",
      "    \n",
      "  File \"/Users/hungtran/code/xai-hack/src/deep_evaluator.py\", line 327, in evaluate\n",
      "    response_text += chunk.content\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hungtran/code/xai-hack/src/deep_evaluator.py\", line 140, in _save_cache\n",
      "    # Convert to dict and handle non-JSON-serializable types\n",
      "  File \"/Users/hungtran/miniconda3/lib/python3.13/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "                 ^^^^^^^^\n",
      "  File \"/Users/hungtran/miniconda3/lib/python3.13/json/encoder.py\", line 432, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/Users/hungtran/miniconda3/lib/python3.13/json/encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/Users/hungtran/miniconda3/lib/python3.13/json/encoder.py\", line 439, in _iterencode\n",
      "    o = _default(o)\n",
      "  File \"/Users/hungtran/miniconda3/lib/python3.13/json/encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "                    f'is not JSON serializable')\n",
      "TypeError: Object of type RepeatedScalarContainer is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "# Initialize deep evaluator and run evaluations\n",
    "try:\n",
    "    deep_evaluator = DeepEvaluator(cache_dir=\"data/enriched\")\n",
    "    print(\"DeepEvaluator initialized successfully\")\n",
    "    print(f\"  Model: {deep_evaluator.model}\")\n",
    "    print(f\"  Cache dir: {deep_evaluator.cache_dir}\")\n",
    "    \n",
    "    # Run deep evaluation\n",
    "    deep_results = deep_evaluator.evaluate_batch(\n",
    "        candidates=candidates_for_deep_eval,\n",
    "        max_candidates=MAX_DEEP_EVAL,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DEEP EVALUATION COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"[error] xai_sdk not installed: {e}\")\n",
    "    print(\"Run: pip install xai-sdk>=1.3.1\")\n",
    "    deep_results = []\n",
    "except Exception as e:\n",
    "    print(f\"[error] Deep evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    deep_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deep evaluation results to export\n"
     ]
    }
   ],
   "source": [
    "# Export deep evaluation results\n",
    "import json as json_module\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "\n",
    "if deep_results:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXPORTING DEEP EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(\"data/enriched\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Export as JSON for API consumption\n",
    "    export_data = []\n",
    "    for evaluation in deep_results:\n",
    "        candidate_data = next(\n",
    "            (c for c in candidates_for_deep_eval if c['handle'] == evaluation.handle),\n",
    "            {}\n",
    "        )\n",
    "        user_id = candidate_data.get('user_id', '')\n",
    "        node_name = builder.nodes[user_id].name if user_id in builder.nodes else ''\n",
    "        \n",
    "        export_data.append({\n",
    "            'handle': evaluation.handle,\n",
    "            'name': node_name,\n",
    "            'bio': evaluation.bio,\n",
    "            'followers_count': evaluation.followers,\n",
    "            'pagerank_score': candidate_data.get('pagerank_score', 0),\n",
    "            'underratedness_score': candidate_data.get('underratedness', 0),\n",
    "            'final_score': evaluation.final_score,\n",
    "            'recommended_role': evaluation.recommended_role,\n",
    "            'summary': evaluation.summary,\n",
    "            'strengths': evaluation.strengths,\n",
    "            'concerns': evaluation.concerns,\n",
    "            'technical_depth': asdict(evaluation.technical_depth),\n",
    "            'project_evidence': asdict(evaluation.project_evidence),\n",
    "            'mission_alignment': asdict(evaluation.mission_alignment),\n",
    "            'exceptional_ability': asdict(evaluation.exceptional_ability),\n",
    "            'communication': asdict(evaluation.communication),\n",
    "            'github_url': evaluation.github_url,\n",
    "            'linkedin_url': evaluation.linkedin_url,\n",
    "            'top_repos': evaluation.top_repos,\n",
    "        })\n",
    "    \n",
    "    export_data.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "    \n",
    "    json_path = output_dir / \"candidates_deep_eval.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json_module.dump(export_data, f, indent=2)\n",
    "    print(f\"Exported to: {json_path}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 candidates:\")\n",
    "    for i, c in enumerate(export_data[:10], 1):\n",
    "        print(f\"{i}. @{c['handle']} - Score: {c['final_score']:.1f} ({c['recommended_role']})\")\n",
    "else:\n",
    "    print(\"No deep evaluation results to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
